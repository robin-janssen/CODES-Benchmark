{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Quickstart\n",
    "\n",
    "This notebook mirrors the CLI quickstart flow in an executable format. Use it to ",
    "validate your environment and to inspect the outputs of a short benchmark run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a configuration\n",
    "\n",
    "Edit `CONFIG_PATH` if you want to point to a custom YAML file. The snippet below ",
    "reads the file so we can re-use the metadata in later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = Path(\"config.yaml\")\n",
    "config = yaml.safe_load(CONFIG_PATH.read_text())\n",
    "config['training_id']"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 2. Trigger training\n",
      "\n",
      "Uncomment the next cell when you want to actually run training from inside the ",
      "notebook. Keeping it as a string prevents accidental long-running jobs when the ",
      "notebook is rendered on the documentation site."
    ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# python run_training.py --config ${CONFIG_PATH} --devices cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark and collect metrics\n",
    "\n",
    "After training finishes, call `run_eval.py` with the same configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# python run_eval.py --config ${CONFIG_PATH} --modes accuracy timing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect generated results\n",
    "\n",
    "Each surrogate writes a YAML summary under `results/<training_id>/<surrogate>/`. ",
    "The cell below loads every summary file and prints the root mean squared error so ",
    "you can compare surrogates quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "results_root = Path(\"results\") / config['training_id']\n",
    "metrics = {}\n",
    "if results_root.exists():\n",
    "    for surrogate_dir in results_root.glob('*'):\n",
    "        summary = surrogate_dir / 'accuracy.yaml'\n",
    "        if summary.exists():\n",
    "            data = yaml.safe_load(summary.read_text())\n",
    "            metrics[surrogate_dir.name] = data.get('accuracy', {}).get('root_mean_squared_error_real')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Use `scripts/download_datasets.py` to fetch additional datasets.\n",
    "* Try enabling `interpolation` or `uncertainty` inside the config file.\n",
    "* Share the notebook with collaborators as a reproducible recipe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
