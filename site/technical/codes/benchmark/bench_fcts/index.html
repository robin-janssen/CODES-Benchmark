<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../../img/favicon.ico" />
    <title>Bench Fcts - Codes-benchmark</title>
    <link rel="stylesheet" href="../../../../css/theme.css" />
    <link rel="stylesheet" href="../../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Bench Fcts";
        var mkdocs_page_input_path = "technical/codes/benchmark/bench_fcts.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../../.." class="icon icon-home"> Codes-benchmark
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../../TODO/">Todos for the project</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Technical</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../../../">Codes-benchmark Index</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" >Codes</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../../">Codes</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" >Benchmark</a>
    <ul class="current">
                <li class="toctree-l3"><a class="reference internal" href="../">Benchmark</a>
                </li>
                <li class="toctree-l3 current"><a class="reference internal current" href="#">Bench Fcts</a>
    <ul class="current">
    <li class="toctree-l4"><a class="reference internal" href="#compare_mae">compare_MAE</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_uq">compare_UQ</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_batchsize">compare_batchsize</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_dynamic_accuracy">compare_dynamic_accuracy</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_extrapolation">compare_extrapolation</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_inference_time">compare_inference_time</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_interpolation">compare_interpolation</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_main_losses">compare_main_losses</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_models">compare_models</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_relative_errors">compare_relative_errors</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#compare_sparse">compare_sparse</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#evaluate_uq">evaluate_UQ</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#evaluate_accuracy">evaluate_accuracy</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#evaluate_batchsize">evaluate_batchsize</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#evaluate_compute">evaluate_compute</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#evaluate_dynamic_accuracy">evaluate_dynamic_accuracy</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#evaluate_extrapolation">evaluate_extrapolation</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#evaluate_interpolation">evaluate_interpolation</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#evaluate_sparse">evaluate_sparse</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#run_benchmark">run_benchmark</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#tabular_comparison">tabular_comparison</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#time_inference">time_inference</a>
    </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../bench_plots/">Bench Plots</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../bench_utils/">Bench Utils</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Surrogates</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../surrogates/">Surrogates</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../surrogates/surrogate_classes/">Surrogate Classes</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../surrogates/surrogates/">Surrogates</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Train</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../train/">Train</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/train_fcts/">Train Fcts</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../utils/">Utils</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../utils/data_utils/">Data Utils</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../utils/utils/">Utils</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Datasets</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" >Data analysis</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../../datasets/data_analysis/">Data Analysis</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../datasets/data_analysis/analyse_dataset/">Analyse Dataset</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../datasets/data_analysis/data_plots/">Data Plots</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../..">Codes-benchmark</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Technical</li>
          <li class="breadcrumb-item">Codes</li>
          <li class="breadcrumb-item">Benchmark</li>
      <li class="breadcrumb-item active">Bench Fcts</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/robin-janssen/CODES-Benchmark/edit/master/docs/technical/codes/benchmark/bench_fcts.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="bench-fcts">Bench Fcts</h1>
<p><a href="../../../#codes-benchmark-index">Codes-benchmark Index</a> / <a href="../../#codes">Codes</a> / <a href="../#benchmark">Benchmark</a> / Bench Fcts</p>
<blockquote>
<p>Auto-generated documentation for <a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py">codes.benchmark.bench_fcts</a> module.</p>
</blockquote>
<ul>
<li><a href="#bench-fcts">Bench Fcts</a></li>
<li><a href="#compare_mae">compare_MAE</a></li>
<li><a href="#compare_uq">compare_UQ</a></li>
<li><a href="#compare_batchsize">compare_batchsize</a></li>
<li><a href="#compare_dynamic_accuracy">compare_dynamic_accuracy</a></li>
<li><a href="#compare_extrapolation">compare_extrapolation</a></li>
<li><a href="#compare_inference_time">compare_inference_time</a></li>
<li><a href="#compare_interpolation">compare_interpolation</a></li>
<li><a href="#compare_main_losses">compare_main_losses</a></li>
<li><a href="#compare_models">compare_models</a></li>
<li><a href="#compare_relative_errors">compare_relative_errors</a></li>
<li><a href="#compare_sparse">compare_sparse</a></li>
<li><a href="#evaluate_uq">evaluate_UQ</a></li>
<li><a href="#evaluate_accuracy">evaluate_accuracy</a></li>
<li><a href="#evaluate_batchsize">evaluate_batchsize</a></li>
<li><a href="#evaluate_compute">evaluate_compute</a></li>
<li><a href="#evaluate_dynamic_accuracy">evaluate_dynamic_accuracy</a></li>
<li><a href="#evaluate_extrapolation">evaluate_extrapolation</a></li>
<li><a href="#evaluate_interpolation">evaluate_interpolation</a></li>
<li><a href="#evaluate_sparse">evaluate_sparse</a></li>
<li><a href="#run_benchmark">run_benchmark</a></li>
<li><a href="#tabular_comparison">tabular_comparison</a></li>
<li><a href="#time_inference">time_inference</a></li>
</ul>
<h2 id="compare_mae">compare_MAE</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L863">Show source in bench_fcts.py:863</a></p>
<p>Compare the MAE of different surrogate models over the course of training.</p>
<h4 id="arguments">Arguments</h4>
<ul>
<li><code>metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns">Returns</h4>
<p>None</p>
<h4 id="signature">Signature</h4>
<pre><code class="language-python">def compare_MAE(metrics: dict, config: dict) -&gt; None: ...
</code></pre>
<h2 id="compare_uq">compare_UQ</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L1113">Show source in bench_fcts.py:1113</a></p>
<p>Compare the uncertainty quantification (UQ) metrics of different surrogate models.</p>
<h4 id="arguments_1">Arguments</h4>
<ul>
<li><code>all_metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns_1">Returns</h4>
<p>None</p>
<h4 id="signature_1">Signature</h4>
<pre><code class="language-python">def compare_UQ(all_metrics: dict, config: dict) -&gt; None: ...
</code></pre>
<h2 id="compare_batchsize">compare_batchsize</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L1082">Show source in bench_fcts.py:1082</a></p>
<p>Compare the batch size training errors of different surrogate models.</p>
<h4 id="arguments_2">Arguments</h4>
<ul>
<li><code>all_metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns_2">Returns</h4>
<p>None</p>
<h4 id="signature_2">Signature</h4>
<pre><code class="language-python">def compare_batchsize(all_metrics: dict, config: dict) -&gt; None: ...
</code></pre>
<h2 id="compare_dynamic_accuracy">compare_dynamic_accuracy</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L960">Show source in bench_fcts.py:960</a></p>
<p>Compare the gradients of different surrogate models.</p>
<h4 id="arguments_3">Arguments</h4>
<ul>
<li><code>metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns_3">Returns</h4>
<p>None</p>
<h4 id="signature_3">Signature</h4>
<pre><code class="language-python">def compare_dynamic_accuracy(metrics: dict, config: dict) -&gt; None: ...
</code></pre>
<h2 id="compare_extrapolation">compare_extrapolation</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L1021">Show source in bench_fcts.py:1021</a></p>
<p>Compare the extrapolation errors of different surrogate models.</p>
<h4 id="arguments_4">Arguments</h4>
<ul>
<li><code>all_metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns_4">Returns</h4>
<p>None</p>
<h4 id="signature_4">Signature</h4>
<pre><code class="language-python">def compare_extrapolation(all_metrics: dict, config: dict) -&gt; None: ...
</code></pre>
<h2 id="compare_inference_time">compare_inference_time</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L928">Show source in bench_fcts.py:928</a></p>
<p>Compare the mean inference time of different surrogate models.</p>
<h4 id="arguments_5">Arguments</h4>
<p>metrics (dict[str, dict]): dictionary containing the benchmark metrics for each surrogate model.
- <code>config</code> <em>dict</em> - Configuration dictionary.
- <code>save</code> <em>bool, optional</em> - Whether to save the plot. Defaults to True.</p>
<h4 id="returns_5">Returns</h4>
<p>None</p>
<h4 id="signature_5">Signature</h4>
<pre><code class="language-python">def compare_inference_time(
    metrics: dict[str, dict], config: dict, save: bool = True
) -&gt; None: ...
</code></pre>
<h2 id="compare_interpolation">compare_interpolation</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L991">Show source in bench_fcts.py:991</a></p>
<p>Compare the interpolation errors of different surrogate models.</p>
<h4 id="arguments_6">Arguments</h4>
<ul>
<li><code>all_metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns_6">Returns</h4>
<p>None</p>
<h4 id="signature_6">Signature</h4>
<pre><code class="language-python">def compare_interpolation(all_metrics: dict, config: dict) -&gt; None: ...
</code></pre>
<h2 id="compare_main_losses">compare_main_losses</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L824">Show source in bench_fcts.py:824</a></p>
<p>Compare the training and test losses of the main models for different surrogate models.</p>
<h4 id="arguments_7">Arguments</h4>
<ul>
<li><code>metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns_7">Returns</h4>
<p>None</p>
<h4 id="signature_7">Signature</h4>
<pre><code class="language-python">def compare_main_losses(metrics: dict, config: dict) -&gt; None: ...
</code></pre>
<h2 id="compare_models">compare_models</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L776">Show source in bench_fcts.py:776</a></p>
<h4 id="signature_8">Signature</h4>
<pre><code class="language-python">def compare_models(metrics: dict, config: dict): ...
</code></pre>
<h2 id="compare_relative_errors">compare_relative_errors</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L897">Show source in bench_fcts.py:897</a></p>
<p>Compare the relative errors over time for different surrogate models.</p>
<h4 id="arguments_8">Arguments</h4>
<ul>
<li><code>metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns_8">Returns</h4>
<p>None</p>
<h4 id="signature_9">Signature</h4>
<pre><code class="language-python">def compare_relative_errors(metrics: dict[str, dict], config: dict) -&gt; None: ...
</code></pre>
<h2 id="compare_sparse">compare_sparse</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L1051">Show source in bench_fcts.py:1051</a></p>
<p>Compare the sparse training errors of different surrogate models.</p>
<h4 id="arguments_9">Arguments</h4>
<ul>
<li><code>all_metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns_9">Returns</h4>
<p>None</p>
<h4 id="signature_10">Signature</h4>
<pre><code class="language-python">def compare_sparse(all_metrics: dict, config: dict) -&gt; None: ...
</code></pre>
<h2 id="evaluate_uq">evaluate_UQ</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L695">Show source in bench_fcts.py:695</a></p>
<p>Evaluate the uncertainty quantification (UQ) performance of the surrogate model.</p>
<h4 id="arguments_10">Arguments</h4>
<ul>
<li><code>model</code> - Instance of the surrogate model class.</li>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model.</li>
<li><code>test_loader</code> <em>DataLoader</em> - The DataLoader object containing the test data.</li>
<li><code>timesteps</code> <em>np.ndarray</em> - The timesteps array.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
<li><code>labels</code> <em>list, optional</em> - The labels for the chemical species.</li>
</ul>
<h4 id="returns_10">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing UQ metrics.</li>
</ul>
<h4 id="signature_11">Signature</h4>
<pre><code class="language-python">def evaluate_UQ(
    model,
    surr_name: str,
    test_loader: DataLoader,
    timesteps: np.ndarray,
    conf: dict,
    labels: list[str] | None = None,
) -&gt; dict[str, Any]: ...
</code></pre>
<h2 id="evaluate_accuracy">evaluate_accuracy</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L173">Show source in bench_fcts.py:173</a></p>
<p>Evaluate the accuracy of the surrogate model.</p>
<h4 id="arguments_11">Arguments</h4>
<ul>
<li><code>model</code> - Instance of the surrogate model class.</li>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model.</li>
<li><code>test_loader</code> <em>DataLoader</em> - The DataLoader object containing the test data.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
<li><code>labels</code> <em>list, optional</em> - The labels for the chemical species.</li>
</ul>
<h4 id="returns_11">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing accuracy metrics.</li>
</ul>
<h4 id="signature_12">Signature</h4>
<pre><code class="language-python">def evaluate_accuracy(
    model,
    surr_name: str,
    test_loader: DataLoader,
    conf: dict,
    labels: list | None = None,
) -&gt; dict[str, Any]: ...
</code></pre>
<h2 id="evaluate_batchsize">evaluate_batchsize</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L632">Show source in bench_fcts.py:632</a></p>
<p>Evaluate the performance of the surrogate model with different batch sizes.</p>
<h4 id="arguments_12">Arguments</h4>
<ul>
<li><code>model</code> - Instance of the surrogate model class.</li>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model.</li>
<li><code>test_loader</code> <em>DataLoader</em> - The DataLoader object containing the test data.</li>
<li><code>timesteps</code> <em>np.ndarray</em> - The timesteps array.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
</ul>
<h4 id="returns_12">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing batch size training metrics.</li>
</ul>
<h4 id="signature_13">Signature</h4>
<pre><code class="language-python">def evaluate_batchsize(
    model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict
) -&gt; dict[str, Any]: ...
</code></pre>
<h2 id="evaluate_compute">evaluate_compute</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L382">Show source in bench_fcts.py:382</a></p>
<p>Evaluate the computational resource requirements of the surrogate model.</p>
<h4 id="arguments_13">Arguments</h4>
<ul>
<li><code>model</code> - Instance of the surrogate model class.</li>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model.</li>
<li><code>test_loader</code> <em>DataLoader</em> - The DataLoader object containing the test data.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
</ul>
<h4 id="returns_13">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing model complexity metrics.</li>
</ul>
<h4 id="signature_14">Signature</h4>
<pre><code class="language-python">def evaluate_compute(
    model, surr_name: str, test_loader: DataLoader, conf: dict
) -&gt; dict[str, Any]: ...
</code></pre>
<h2 id="evaluate_dynamic_accuracy">evaluate_dynamic_accuracy</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L248">Show source in bench_fcts.py:248</a></p>
<p>Evaluate the gradients of the surrogate model.</p>
<h4 id="arguments_14">Arguments</h4>
<ul>
<li><code>model</code> - Instance of the surrogate model class.</li>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model.</li>
<li><code>test_loader</code> <em>DataLoader</em> - The DataLoader object containing the test data.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
</ul>
<h4 id="returns_14">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing gradients metrics.</li>
</ul>
<h4 id="signature_15">Signature</h4>
<pre><code class="language-python">def evaluate_dynamic_accuracy(
    model,
    surr_name: str,
    test_loader: DataLoader,
    conf: dict,
    species_names: list = None,
) -&gt; dict: ...
</code></pre>
<h2 id="evaluate_extrapolation">evaluate_extrapolation</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L485">Show source in bench_fcts.py:485</a></p>
<p>Evaluate the extrapolation performance of the surrogate model.</p>
<h4 id="arguments_15">Arguments</h4>
<ul>
<li><code>model</code> - Instance of the surrogate model class.</li>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model.</li>
<li><code>test_loader</code> <em>DataLoader</em> - The DataLoader object containing the test data.</li>
<li><code>timesteps</code> <em>np.ndarray</em> - The timesteps array.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
</ul>
<h4 id="returns_15">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing extrapolation metrics.</li>
</ul>
<h4 id="signature_16">Signature</h4>
<pre><code class="language-python">def evaluate_extrapolation(
    model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict
) -&gt; dict[str, Any]: ...
</code></pre>
<h2 id="evaluate_interpolation">evaluate_interpolation</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L417">Show source in bench_fcts.py:417</a></p>
<p>Evaluate the interpolation performance of the surrogate model.</p>
<h4 id="arguments_16">Arguments</h4>
<ul>
<li><code>model</code> - Instance of the surrogate model class.</li>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model.</li>
<li><code>test_loader</code> <em>DataLoader</em> - The DataLoader object containing the test data.</li>
<li><code>timesteps</code> <em>np.ndarray</em> - The timesteps array.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
</ul>
<h4 id="returns_16">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing interpolation metrics.</li>
</ul>
<h4 id="signature_17">Signature</h4>
<pre><code class="language-python">def evaluate_interpolation(
    model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict
) -&gt; dict[str, Any]: ...
</code></pre>
<h2 id="evaluate_sparse">evaluate_sparse</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L554">Show source in bench_fcts.py:554</a></p>
<p>Evaluate the performance of the surrogate model with sparse training data.</p>
<h4 id="arguments_17">Arguments</h4>
<ul>
<li><code>model</code> - Instance of the surrogate model class.</li>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model.</li>
<li><code>test_loader</code> <em>DataLoader</em> - The DataLoader object containing the test data.</li>
<li><code>n_train_samples</code> <em>int</em> - The number of training samples in the full dataset.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
</ul>
<h4 id="returns_17">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing sparse training metrics.</li>
</ul>
<h4 id="signature_18">Signature</h4>
<pre><code class="language-python">def evaluate_sparse(
    model,
    surr_name: str,
    test_loader: DataLoader,
    timesteps: np.ndarray,
    n_train_samples: int,
    conf: dict,
) -&gt; dict[str, Any]: ...
</code></pre>
<h2 id="run_benchmark">run_benchmark</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L47">Show source in bench_fcts.py:47</a></p>
<p>Run benchmarks for a given surrogate model.</p>
<h4 id="arguments_18">Arguments</h4>
<ul>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model to benchmark.</li>
<li><code>surrogate_class</code> - The class of the surrogate model.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
</ul>
<h4 id="returns_18">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing all relevant metrics for the given model.</li>
</ul>
<h4 id="signature_19">Signature</h4>
<pre><code class="language-python">def run_benchmark(surr_name: str, surrogate_class, conf: dict) -&gt; dict[str, Any]: ...
</code></pre>
<h2 id="tabular_comparison">tabular_comparison</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L1146">Show source in bench_fcts.py:1146</a></p>
<p>Compare the metrics of different surrogate models in a tabular format.</p>
<h4 id="arguments_19">Arguments</h4>
<ul>
<li><code>all_metrics</code> <em>dict</em> - dictionary containing the benchmark metrics for each surrogate model.</li>
<li><code>config</code> <em>dict</em> - Configuration dictionary.</li>
</ul>
<h4 id="returns_19">Returns</h4>
<p>None</p>
<h4 id="signature_20">Signature</h4>
<pre><code class="language-python">def tabular_comparison(all_metrics: dict, config: dict) -&gt; None: ...
</code></pre>
<h2 id="time_inference">time_inference</h2>
<p><a href="https://github.com/robin-janssen/CODES-Benchmark/blob/main/codes/benchmark/bench_fcts.py#L326">Show source in bench_fcts.py:326</a></p>
<p>Time the inference of the surrogate model.</p>
<h4 id="arguments_20">Arguments</h4>
<ul>
<li><code>model</code> - Instance of the surrogate model class.</li>
<li><code>surr_name</code> <em>str</em> - The name of the surrogate model.</li>
<li><code>test_loader</code> <em>DataLoader</em> - The DataLoader object containing the test data.</li>
<li><code>timesteps</code> <em>np.ndarray</em> - The timesteps array.</li>
<li><code>conf</code> <em>dict</em> - The configuration dictionary.</li>
<li><code>n_test_samples</code> <em>int</em> - The number of test samples.</li>
<li><code>n_runs</code> <em>int, optional</em> - Number of times to run the inference for timing.</li>
</ul>
<h4 id="returns_20">Returns</h4>
<ul>
<li><code>dict</code> - A dictionary containing timing metrics.</li>
</ul>
<h4 id="signature_21">Signature</h4>
<pre><code class="language-python">def time_inference(
    model,
    surr_name: str,
    test_loader: DataLoader,
    conf: dict,
    n_test_samples: int,
    n_runs: int = 5,
) -&gt; dict[str, Any]: ...
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../" class="btn btn-neutral float-left" title="Benchmark"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../bench_plots/" class="btn btn-neutral float-right" title="Bench Plots">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/robin-janssen/CODES-Benchmark" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../bench_plots/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../../..";</script>
    <script src="../../../../js/theme_extra.js"></script>
    <script src="../../../../js/theme.js"></script>
      <script src="../../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
