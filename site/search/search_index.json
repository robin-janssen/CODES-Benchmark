{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"TODO/","text":"Todos for the project [x] Write NeurIPS submission [x] Read into DataHosting on HeiDATA -> use Zenodo for now [x] Also upload other datasets and create download function [x] Choose a code licence for the code - MIT? [x] Reach out to Lorenzo regarding the dataset [ ] Make a ruleset for the main branch [ ] Optuna and final run for all datasets. [ ] Setup repository: Copy over actions and other settings from the SSC Cookiecutter Template [ ] Add links in the documentation to respective other sections [x] Rename main scripts on the website (run_benchmark.py, run_training.py) Refactor [x] Remove the # model.n_timesteps = 100 lines in bench_fcts once new models are trained. [x] Find a catchy name - CODES Benchmark: Coupled ODE System Surrogates [x] Think about the organisation of the model config files [x] Complete the README.md [x] Rename NeuralODE to LatentNeuralODE [x] Check the time benchmarking [x] Check whether the training is indeed deterministic. [x] Implement optional log scale labels in plots [ ] Check the memory benchmarking [x] Docstrings completion. [x] Make a clean requirements.txt [x] Use poetry for packages [ ] Add alive progress bars instead of tqdm [x] Make a develop branch [x] Add a .lock file to ensure full reproducibility of all dependencies [ ] Check the documentation and utility functions for compatibility with the refactoring (src/codes) [ ] Make more docs: Contribution guidelines, pull requests, etc. [ ] Avoid having to put self.setup_progress_bar in the fit function manually. [ ] Add mps support for LatentPoly and LatentNeuralODE (use float32 instead of float64). [ ] Save Plots as SVG for immaculate quality [ ] Do a fresh test install of the repo and verify everything is running [ ] Check/update the config maker [ ] Add testing (for workshop) [ ] Fix the task list for faulty runs in the benchmark [ ] Rename batch_scaling to batch. [ ] Think about the error plots - where should absolute errors be used, where relative errors? Does it make sense to use relative errors for the chemical error plots? [ ] Make tutorial notebooks/examples New Features [x] Javascript config maker [x] Make a table in the CLI at the end of the benchmark with the most important metrics [x] Make a csv file with the most important metrics [x] Make a json file with the most important metrics [x] Add and document Lorenzo's Dataset [x] Optuna tuning script [x] Calculate and add error quantities per surrogate to the individal (and comparative) outputs. [x] Add user prompt on whether to use task list or overwrite it. [x] Add additional baseline datasets -> implement dynamic datasets [ ] Add support for flexible timesteps: The number of timesteps and their intervals should be unique to each sample. [ ] Support a timesteps array for each dataset [ ] Time estimation using training duration of the main models an effective number of models to train. [ ] Store output in a .txt file [ ] Refactoring for more generality (remove chemistry specific code) [ ] Continue training of the existing models in case further convergence is needed [ ] Accumulation of error when predicting quantities iteratively [ ] Inter-/extrapolation in the initial conditions (domain shift) [ ] Dataset Visualizations - also include how the distribution changes over time (for iterative preds) [ ] Determine optimal model parameters for the baseline models per dataset [ ] Integrate Torch Compile for potentially better performance [ ] Add Robertson system to datasets as a toy problem Potential Models [ ] Multiple DeepONets [ ] NeuralODE without autoencoder [ ] LSTM [ ] SINDy [ ] Chemulator [ ] Echo state networks Plots [x] Heatmaps comparative plot [x] Layout of chemical error distribution plot similar to example UQ plots [x] Add overall quantities to the plots (e.g. mean, std, etc.) [ ] Add the name of the surrogate and dataset to the error_dist per quantitiy and mention relative errors in title and axis labels [ ] Add \"absolute\" to the y axis of the uq heatmaps plot [ ] Change the figsize of the heatmap plots [ ] Contour plots to compare dynamics/UQ correlations [ ] Average prediction error over time + average gradient over time [ ] Scatter plot of absolute/relative error vs. inference time comparing surrogates.","title":"Todos for the project"},{"location":"TODO/#todos-for-the-project","text":"[x] Write NeurIPS submission [x] Read into DataHosting on HeiDATA -> use Zenodo for now [x] Also upload other datasets and create download function [x] Choose a code licence for the code - MIT? [x] Reach out to Lorenzo regarding the dataset [ ] Make a ruleset for the main branch [ ] Optuna and final run for all datasets. [ ] Setup repository: Copy over actions and other settings from the SSC Cookiecutter Template [ ] Add links in the documentation to respective other sections [x] Rename main scripts on the website (run_benchmark.py, run_training.py)","title":"Todos for the project"},{"location":"TODO/#refactor","text":"[x] Remove the # model.n_timesteps = 100 lines in bench_fcts once new models are trained. [x] Find a catchy name - CODES Benchmark: Coupled ODE System Surrogates [x] Think about the organisation of the model config files [x] Complete the README.md [x] Rename NeuralODE to LatentNeuralODE [x] Check the time benchmarking [x] Check whether the training is indeed deterministic. [x] Implement optional log scale labels in plots [ ] Check the memory benchmarking [x] Docstrings completion. [x] Make a clean requirements.txt [x] Use poetry for packages [ ] Add alive progress bars instead of tqdm [x] Make a develop branch [x] Add a .lock file to ensure full reproducibility of all dependencies [ ] Check the documentation and utility functions for compatibility with the refactoring (src/codes) [ ] Make more docs: Contribution guidelines, pull requests, etc. [ ] Avoid having to put self.setup_progress_bar in the fit function manually. [ ] Add mps support for LatentPoly and LatentNeuralODE (use float32 instead of float64). [ ] Save Plots as SVG for immaculate quality [ ] Do a fresh test install of the repo and verify everything is running [ ] Check/update the config maker [ ] Add testing (for workshop) [ ] Fix the task list for faulty runs in the benchmark [ ] Rename batch_scaling to batch. [ ] Think about the error plots - where should absolute errors be used, where relative errors? Does it make sense to use relative errors for the chemical error plots? [ ] Make tutorial notebooks/examples","title":"Refactor"},{"location":"TODO/#new-features","text":"[x] Javascript config maker [x] Make a table in the CLI at the end of the benchmark with the most important metrics [x] Make a csv file with the most important metrics [x] Make a json file with the most important metrics [x] Add and document Lorenzo's Dataset [x] Optuna tuning script [x] Calculate and add error quantities per surrogate to the individal (and comparative) outputs. [x] Add user prompt on whether to use task list or overwrite it. [x] Add additional baseline datasets -> implement dynamic datasets [ ] Add support for flexible timesteps: The number of timesteps and their intervals should be unique to each sample. [ ] Support a timesteps array for each dataset [ ] Time estimation using training duration of the main models an effective number of models to train. [ ] Store output in a .txt file [ ] Refactoring for more generality (remove chemistry specific code) [ ] Continue training of the existing models in case further convergence is needed [ ] Accumulation of error when predicting quantities iteratively [ ] Inter-/extrapolation in the initial conditions (domain shift) [ ] Dataset Visualizations - also include how the distribution changes over time (for iterative preds) [ ] Determine optimal model parameters for the baseline models per dataset [ ] Integrate Torch Compile for potentially better performance [ ] Add Robertson system to datasets as a toy problem","title":"New Features"},{"location":"TODO/#potential-models","text":"[ ] Multiple DeepONets [ ] NeuralODE without autoencoder [ ] LSTM [ ] SINDy [ ] Chemulator [ ] Echo state networks","title":"Potential Models"},{"location":"TODO/#plots","text":"[x] Heatmaps comparative plot [x] Layout of chemical error distribution plot similar to example UQ plots [x] Add overall quantities to the plots (e.g. mean, std, etc.) [ ] Add the name of the surrogate and dataset to the error_dist per quantitiy and mention relative errors in title and axis labels [ ] Add \"absolute\" to the y axis of the uq heatmaps plot [ ] Change the figsize of the heatmap plots [ ] Contour plots to compare dynamics/UQ correlations [ ] Average prediction error over time + average gradient over time [ ] Scatter plot of absolute/relative error vs. inference time comparing surrogates.","title":"Plots"},{"location":"technical/","text":"Codes-benchmark Index Auto-generated documentation index. A full list of Codes-benchmark project modules. Codes Benchmark Bench Fcts Bench Plots Bench Utils Surrogates Surrogate Classes Surrogates Train Train Fcts Utils Data Utils Utils Data Analysis Analyse Dataset Data Plots","title":"Codes-benchmark Index"},{"location":"technical/#codes-benchmark-index","text":"Auto-generated documentation index. A full list of Codes-benchmark project modules. Codes Benchmark Bench Fcts Bench Plots Bench Utils Surrogates Surrogate Classes Surrogates Train Train Fcts Utils Data Utils Utils Data Analysis Analyse Dataset Data Plots","title":"Codes-benchmark Index"},{"location":"technical/codes/","text":"Codes Codes-benchmark Index / Codes Auto-generated documentation for codes module. Codes Modules Modules Benchmark Surrogates Train Utils","title":"Codes"},{"location":"technical/codes/#codes","text":"Codes-benchmark Index / Codes Auto-generated documentation for codes module. Codes Modules","title":"Codes"},{"location":"technical/codes/#modules","text":"Benchmark Surrogates Train Utils","title":"Modules"},{"location":"technical/codes/benchmark/","text":"Benchmark Codes-benchmark Index / Codes / Benchmark Auto-generated documentation for codes.benchmark module. Benchmark Modules Modules Bench Fcts Bench Plots Bench Utils","title":"Benchmark"},{"location":"technical/codes/benchmark/#benchmark","text":"Codes-benchmark Index / Codes / Benchmark Auto-generated documentation for codes.benchmark module. Benchmark Modules","title":"Benchmark"},{"location":"technical/codes/benchmark/#modules","text":"Bench Fcts Bench Plots Bench Utils","title":"Modules"},{"location":"technical/codes/benchmark/bench_fcts/","text":"Bench Fcts Codes-benchmark Index / Codes / Benchmark / Bench Fcts Auto-generated documentation for codes.benchmark.bench_fcts module. Bench Fcts compare_MAE compare_UQ compare_batchsize compare_dynamic_accuracy compare_extrapolation compare_inference_time compare_interpolation compare_main_losses compare_models compare_relative_errors compare_sparse evaluate_UQ evaluate_accuracy evaluate_batchsize evaluate_compute evaluate_dynamic_accuracy evaluate_extrapolation evaluate_interpolation evaluate_sparse run_benchmark tabular_comparison time_inference compare_MAE Show source in bench_fcts.py:863 Compare the MAE of different surrogate models over the course of training. Arguments metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def compare_MAE(metrics: dict, config: dict) -> None: ... compare_UQ Show source in bench_fcts.py:1113 Compare the uncertainty quantification (UQ) metrics of different surrogate models. Arguments all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def compare_UQ(all_metrics: dict, config: dict) -> None: ... compare_batchsize Show source in bench_fcts.py:1082 Compare the batch size training errors of different surrogate models. Arguments all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def compare_batchsize(all_metrics: dict, config: dict) -> None: ... compare_dynamic_accuracy Show source in bench_fcts.py:960 Compare the gradients of different surrogate models. Arguments metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def compare_dynamic_accuracy(metrics: dict, config: dict) -> None: ... compare_extrapolation Show source in bench_fcts.py:1021 Compare the extrapolation errors of different surrogate models. Arguments all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def compare_extrapolation(all_metrics: dict, config: dict) -> None: ... compare_inference_time Show source in bench_fcts.py:928 Compare the mean inference time of different surrogate models. Arguments metrics (dict[str, dict]): dictionary containing the benchmark metrics for each surrogate model. - config dict - Configuration dictionary. - save bool, optional - Whether to save the plot. Defaults to True. Returns None Signature def compare_inference_time( metrics: dict[str, dict], config: dict, save: bool = True ) -> None: ... compare_interpolation Show source in bench_fcts.py:991 Compare the interpolation errors of different surrogate models. Arguments all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def compare_interpolation(all_metrics: dict, config: dict) -> None: ... compare_main_losses Show source in bench_fcts.py:824 Compare the training and test losses of the main models for different surrogate models. Arguments metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def compare_main_losses(metrics: dict, config: dict) -> None: ... compare_models Show source in bench_fcts.py:776 Signature def compare_models(metrics: dict, config: dict): ... compare_relative_errors Show source in bench_fcts.py:897 Compare the relative errors over time for different surrogate models. Arguments metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def compare_relative_errors(metrics: dict[str, dict], config: dict) -> None: ... compare_sparse Show source in bench_fcts.py:1051 Compare the sparse training errors of different surrogate models. Arguments all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def compare_sparse(all_metrics: dict, config: dict) -> None: ... evaluate_UQ Show source in bench_fcts.py:695 Evaluate the uncertainty quantification (UQ) performance of the surrogate model. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary. labels list, optional - The labels for the chemical species. Returns dict - A dictionary containing UQ metrics. Signature def evaluate_UQ( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict, labels: list[str] | None = None, ) -> dict[str, Any]: ... evaluate_accuracy Show source in bench_fcts.py:173 Evaluate the accuracy of the surrogate model. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. conf dict - The configuration dictionary. labels list, optional - The labels for the chemical species. Returns dict - A dictionary containing accuracy metrics. Signature def evaluate_accuracy( model, surr_name: str, test_loader: DataLoader, conf: dict, labels: list | None = None, ) -> dict[str, Any]: ... evaluate_batchsize Show source in bench_fcts.py:632 Evaluate the performance of the surrogate model with different batch sizes. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary. Returns dict - A dictionary containing batch size training metrics. Signature def evaluate_batchsize( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict ) -> dict[str, Any]: ... evaluate_compute Show source in bench_fcts.py:382 Evaluate the computational resource requirements of the surrogate model. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. conf dict - The configuration dictionary. Returns dict - A dictionary containing model complexity metrics. Signature def evaluate_compute( model, surr_name: str, test_loader: DataLoader, conf: dict ) -> dict[str, Any]: ... evaluate_dynamic_accuracy Show source in bench_fcts.py:248 Evaluate the gradients of the surrogate model. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. conf dict - The configuration dictionary. Returns dict - A dictionary containing gradients metrics. Signature def evaluate_dynamic_accuracy( model, surr_name: str, test_loader: DataLoader, conf: dict, species_names: list = None, ) -> dict: ... evaluate_extrapolation Show source in bench_fcts.py:485 Evaluate the extrapolation performance of the surrogate model. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary. Returns dict - A dictionary containing extrapolation metrics. Signature def evaluate_extrapolation( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict ) -> dict[str, Any]: ... evaluate_interpolation Show source in bench_fcts.py:417 Evaluate the interpolation performance of the surrogate model. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary. Returns dict - A dictionary containing interpolation metrics. Signature def evaluate_interpolation( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict ) -> dict[str, Any]: ... evaluate_sparse Show source in bench_fcts.py:554 Evaluate the performance of the surrogate model with sparse training data. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. n_train_samples int - The number of training samples in the full dataset. conf dict - The configuration dictionary. Returns dict - A dictionary containing sparse training metrics. Signature def evaluate_sparse( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, n_train_samples: int, conf: dict, ) -> dict[str, Any]: ... run_benchmark Show source in bench_fcts.py:47 Run benchmarks for a given surrogate model. Arguments surr_name str - The name of the surrogate model to benchmark. surrogate_class - The class of the surrogate model. conf dict - The configuration dictionary. Returns dict - A dictionary containing all relevant metrics for the given model. Signature def run_benchmark(surr_name: str, surrogate_class, conf: dict) -> dict[str, Any]: ... tabular_comparison Show source in bench_fcts.py:1146 Compare the metrics of different surrogate models in a tabular format. Arguments all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def tabular_comparison(all_metrics: dict, config: dict) -> None: ... time_inference Show source in bench_fcts.py:326 Time the inference of the surrogate model. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary. n_test_samples int - The number of test samples. n_runs int, optional - Number of times to run the inference for timing. Returns dict - A dictionary containing timing metrics. Signature def time_inference( model, surr_name: str, test_loader: DataLoader, conf: dict, n_test_samples: int, n_runs: int = 5, ) -> dict[str, Any]: ...","title":"Bench Fcts"},{"location":"technical/codes/benchmark/bench_fcts/#bench-fcts","text":"Codes-benchmark Index / Codes / Benchmark / Bench Fcts Auto-generated documentation for codes.benchmark.bench_fcts module. Bench Fcts compare_MAE compare_UQ compare_batchsize compare_dynamic_accuracy compare_extrapolation compare_inference_time compare_interpolation compare_main_losses compare_models compare_relative_errors compare_sparse evaluate_UQ evaluate_accuracy evaluate_batchsize evaluate_compute evaluate_dynamic_accuracy evaluate_extrapolation evaluate_interpolation evaluate_sparse run_benchmark tabular_comparison time_inference","title":"Bench Fcts"},{"location":"technical/codes/benchmark/bench_fcts/#compare_mae","text":"Show source in bench_fcts.py:863 Compare the MAE of different surrogate models over the course of training.","title":"compare_MAE"},{"location":"technical/codes/benchmark/bench_fcts/#arguments","text":"metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature","text":"def compare_MAE(metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_uq","text":"Show source in bench_fcts.py:1113 Compare the uncertainty quantification (UQ) metrics of different surrogate models.","title":"compare_UQ"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_1","text":"all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_1","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_1","text":"def compare_UQ(all_metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_batchsize","text":"Show source in bench_fcts.py:1082 Compare the batch size training errors of different surrogate models.","title":"compare_batchsize"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_2","text":"all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_2","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_2","text":"def compare_batchsize(all_metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_dynamic_accuracy","text":"Show source in bench_fcts.py:960 Compare the gradients of different surrogate models.","title":"compare_dynamic_accuracy"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_3","text":"metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_3","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_3","text":"def compare_dynamic_accuracy(metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_extrapolation","text":"Show source in bench_fcts.py:1021 Compare the extrapolation errors of different surrogate models.","title":"compare_extrapolation"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_4","text":"all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_4","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_4","text":"def compare_extrapolation(all_metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_inference_time","text":"Show source in bench_fcts.py:928 Compare the mean inference time of different surrogate models.","title":"compare_inference_time"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_5","text":"metrics (dict[str, dict]): dictionary containing the benchmark metrics for each surrogate model. - config dict - Configuration dictionary. - save bool, optional - Whether to save the plot. Defaults to True.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_5","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_5","text":"def compare_inference_time( metrics: dict[str, dict], config: dict, save: bool = True ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_interpolation","text":"Show source in bench_fcts.py:991 Compare the interpolation errors of different surrogate models.","title":"compare_interpolation"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_6","text":"all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_6","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_6","text":"def compare_interpolation(all_metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_main_losses","text":"Show source in bench_fcts.py:824 Compare the training and test losses of the main models for different surrogate models.","title":"compare_main_losses"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_7","text":"metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_7","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_7","text":"def compare_main_losses(metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_models","text":"Show source in bench_fcts.py:776","title":"compare_models"},{"location":"technical/codes/benchmark/bench_fcts/#signature_8","text":"def compare_models(metrics: dict, config: dict): ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_relative_errors","text":"Show source in bench_fcts.py:897 Compare the relative errors over time for different surrogate models.","title":"compare_relative_errors"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_8","text":"metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_8","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_9","text":"def compare_relative_errors(metrics: dict[str, dict], config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#compare_sparse","text":"Show source in bench_fcts.py:1051 Compare the sparse training errors of different surrogate models.","title":"compare_sparse"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_9","text":"all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_9","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_10","text":"def compare_sparse(all_metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#evaluate_uq","text":"Show source in bench_fcts.py:695 Evaluate the uncertainty quantification (UQ) performance of the surrogate model.","title":"evaluate_UQ"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_10","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary. labels list, optional - The labels for the chemical species.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_10","text":"dict - A dictionary containing UQ metrics.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_11","text":"def evaluate_UQ( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict, labels: list[str] | None = None, ) -> dict[str, Any]: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#evaluate_accuracy","text":"Show source in bench_fcts.py:173 Evaluate the accuracy of the surrogate model.","title":"evaluate_accuracy"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_11","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. conf dict - The configuration dictionary. labels list, optional - The labels for the chemical species.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_11","text":"dict - A dictionary containing accuracy metrics.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_12","text":"def evaluate_accuracy( model, surr_name: str, test_loader: DataLoader, conf: dict, labels: list | None = None, ) -> dict[str, Any]: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#evaluate_batchsize","text":"Show source in bench_fcts.py:632 Evaluate the performance of the surrogate model with different batch sizes.","title":"evaluate_batchsize"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_12","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_12","text":"dict - A dictionary containing batch size training metrics.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_13","text":"def evaluate_batchsize( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict ) -> dict[str, Any]: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#evaluate_compute","text":"Show source in bench_fcts.py:382 Evaluate the computational resource requirements of the surrogate model.","title":"evaluate_compute"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_13","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_13","text":"dict - A dictionary containing model complexity metrics.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_14","text":"def evaluate_compute( model, surr_name: str, test_loader: DataLoader, conf: dict ) -> dict[str, Any]: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#evaluate_dynamic_accuracy","text":"Show source in bench_fcts.py:248 Evaluate the gradients of the surrogate model.","title":"evaluate_dynamic_accuracy"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_14","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_14","text":"dict - A dictionary containing gradients metrics.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_15","text":"def evaluate_dynamic_accuracy( model, surr_name: str, test_loader: DataLoader, conf: dict, species_names: list = None, ) -> dict: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#evaluate_extrapolation","text":"Show source in bench_fcts.py:485 Evaluate the extrapolation performance of the surrogate model.","title":"evaluate_extrapolation"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_15","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_15","text":"dict - A dictionary containing extrapolation metrics.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_16","text":"def evaluate_extrapolation( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict ) -> dict[str, Any]: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#evaluate_interpolation","text":"Show source in bench_fcts.py:417 Evaluate the interpolation performance of the surrogate model.","title":"evaluate_interpolation"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_16","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_16","text":"dict - A dictionary containing interpolation metrics.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_17","text":"def evaluate_interpolation( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, conf: dict ) -> dict[str, Any]: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#evaluate_sparse","text":"Show source in bench_fcts.py:554 Evaluate the performance of the surrogate model with sparse training data.","title":"evaluate_sparse"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_17","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. n_train_samples int - The number of training samples in the full dataset. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_17","text":"dict - A dictionary containing sparse training metrics.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_18","text":"def evaluate_sparse( model, surr_name: str, test_loader: DataLoader, timesteps: np.ndarray, n_train_samples: int, conf: dict, ) -> dict[str, Any]: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#run_benchmark","text":"Show source in bench_fcts.py:47 Run benchmarks for a given surrogate model.","title":"run_benchmark"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_18","text":"surr_name str - The name of the surrogate model to benchmark. surrogate_class - The class of the surrogate model. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_18","text":"dict - A dictionary containing all relevant metrics for the given model.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_19","text":"def run_benchmark(surr_name: str, surrogate_class, conf: dict) -> dict[str, Any]: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#tabular_comparison","text":"Show source in bench_fcts.py:1146 Compare the metrics of different surrogate models in a tabular format.","title":"tabular_comparison"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_19","text":"all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_19","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_20","text":"def tabular_comparison(all_metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_fcts/#time_inference","text":"Show source in bench_fcts.py:326 Time the inference of the surrogate model.","title":"time_inference"},{"location":"technical/codes/benchmark/bench_fcts/#arguments_20","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. test_loader DataLoader - The DataLoader object containing the test data. timesteps np.ndarray - The timesteps array. conf dict - The configuration dictionary. n_test_samples int - The number of test samples. n_runs int, optional - Number of times to run the inference for timing.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_fcts/#returns_20","text":"dict - A dictionary containing timing metrics.","title":"Returns"},{"location":"technical/codes/benchmark/bench_fcts/#signature_21","text":"def time_inference( model, surr_name: str, test_loader: DataLoader, conf: dict, n_test_samples: int, n_runs: int = 5, ) -> dict[str, Any]: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/","text":"Bench Plots Codes-benchmark Index / Codes / Benchmark / Bench Plots Auto-generated documentation for codes.benchmark.bench_plots module. Bench Plots get_custom_palette inference_time_bar_plot int_ext_sparse plot_MAE_comparison plot_MAE_comparison_train_duration plot_average_errors_over_time plot_average_uncertainty_over_time plot_comparative_dynamic_correlation_heatmaps plot_comparative_error_correlation_heatmaps plot_dynamic_correlation plot_dynamic_correlation_heatmap plot_error_correlation_heatmap plot_error_distribution_comparative plot_error_distribution_per_chemical plot_example_predictions_with_uncertainty plot_generalization_error_comparison plot_generalization_errors plot_loss_comparison plot_losses plot_relative_errors plot_relative_errors_over_time plot_surr_losses plot_uncertainty_over_time_comparison plot_uncertainty_vs_errors rel_errors_and_uq save_plot save_plot_counter get_custom_palette Show source in bench_plots.py:1645 Returns a list of colors sampled from a custom color palette. Arguments num_colors int - The number of colors needed. Returns list - A list of RGBA color tuples. Signature def get_custom_palette(num_colors): ... inference_time_bar_plot Show source in bench_plots.py:1065 Plot the mean inference time with standard deviation for different surrogate models. Arguments surrogates List[str] - List of surrogate model names. means List[float] - List of mean inference times for each surrogate model. stds List[float] - List of standard deviation of inference times for each surrogate model. config dict - Configuration dictionary. save bool, optional - Whether to save the plot. Defaults to True. Returns None Signature def inference_time_bar_plot( surrogates: list[str], means: list[float], stds: list[float], config: dict, save: bool = True, ) -> None: ... int_ext_sparse Show source in bench_plots.py:1691 Function to make one comparative plot of the interpolation, extrapolation, and sparse training errors. Arguments all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def int_ext_sparse(all_metrics: dict, config: dict) -> None: ... plot_MAE_comparison Show source in bench_plots.py:873 Plot the MAE for different surrogate models. Arguments MAE tuple - Tuple of accuracy arrays for each surrogate model. labels tuple - Tuple of labels for each surrogate model. config dict - Configuration dictionary. save bool - Whether to save the plot. Signature def plot_MAE_comparison( MAEs: tuple[np.ndarray, ...], labels: tuple[str, ...], config: dict, save: bool = True, ) -> None: ... plot_MAE_comparison_train_duration Show source in bench_plots.py:910 Plot the MAE for different surrogate models. Arguments MAE tuple - Tuple of accuracy arrays for each surrogate model. labels tuple - Tuple of labels for each surrogate model. config dict - Configuration dictionary. save bool - Whether to save the plot. Signature def plot_MAE_comparison_train_duration( MAEs: tuple[np.ndarray, ...], labels: tuple[str, ...], train_durations: tuple[float, ...], config: dict, save: bool = True, ) -> None: ... plot_average_errors_over_time Show source in bench_plots.py:258 Plot the errors over time for different modes (interpolation, extrapolation, sparse, batchsize). Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. errors np.ndarray - Errors array of shape [N_metrics, n_timesteps]. metrics np.ndarray - Metrics array of shape [N_metrics]. timesteps np.ndarray - Timesteps array. mode str - The mode of evaluation ('interpolation', 'extrapolation', 'sparse', 'batchsize'). save bool, optional - Whether to save the plot as a file. Signature def plot_average_errors_over_time( surr_name: str, conf: dict, errors: np.ndarray, metrics: np.ndarray, timesteps: np.ndarray, mode: str, save: bool = False, ) -> None: ... plot_average_uncertainty_over_time Show source in bench_plots.py:455 Plot the average uncertainty over time. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. errors_time np.ndarray - Prediction errors over time. preds_std np.ndarray - Standard deviation over time of predictions from the ensemble of models. timesteps np.ndarray - Timesteps array. save bool, optional - Whether to save the plot as a file. Signature def plot_average_uncertainty_over_time( surr_name: str, conf: dict, errors_time: np.ndarray, preds_std: np.ndarray, timesteps: np.ndarray, save: bool = False, ) -> None: ... plot_comparative_dynamic_correlation_heatmaps Show source in bench_plots.py:1550 Plot comparative heatmaps of correlation between gradient and prediction errors for multiple surrogate models. Arguments gradients (dict[str, np.ndarray]): Dictionary of gradients from the ensemble of models. errors (dict[str, np.ndarray]): Dictionary of prediction errors. avg_correlations (dict[str, float]): Dictionary of average correlations between gradients and prediction errors. max_grad (dict[str, float]): Dictionary of maximum gradient values for axis scaling across models. max_err (dict[str, float]): Dictionary of maximum error values for axis scaling across models. max_count (dict[str, float]): Dictionary of maximum count values for heatmap normalization across models. - config dict - Configuration dictionary. - save bool, optional - Whether to save the plot. Defaults to True. Returns None Signature def plot_comparative_dynamic_correlation_heatmaps( gradients: dict[str, np.ndarray], errors: dict[str, np.ndarray], avg_correlations: dict[str, float], max_grad: dict[str, float], max_err: dict[str, float], max_count: dict[str, float], config: dict, save: bool = True, ) -> None: ... plot_comparative_error_correlation_heatmaps Show source in bench_plots.py:1453 Plot comparative heatmaps of correlation between predictive uncertainty and prediction errors for multiple surrogate models. Arguments preds_std (dict[str, np.ndarray]): Dictionary of standard deviation of predictions from the ensemble of models. errors (dict[str, np.ndarray]): Dictionary of prediction errors. avg_correlations (dict[str, float]): Dictionary of average correlations between gradients and prediction errors. axis_max (dict[str, float]): Dictionary of maximum values for axis scaling across models. max_count (dict[str, float]): Dictionary of maximum count values for heatmap normalization across models. - config dict - Configuration dictionary. - save bool, optional - Whether to save the plot. Defaults to True. Returns None Signature def plot_comparative_error_correlation_heatmaps( preds_std: dict[str, np.ndarray], errors: dict[str, np.ndarray], avg_correlations: dict[str, float], axis_max: dict[str, float], max_count: dict[str, float], config: dict, save: bool = True, ) -> None: ... plot_dynamic_correlation Show source in bench_plots.py:163 Plot the correlation between the gradients of the data and the prediction errors. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. gradients np.ndarray - The gradients of the data. errors np.ndarray - The prediction errors. save bool - Whether to save the plot. Signature def plot_dynamic_correlation( surr_name: str, conf: dict, gradients: np.ndarray, errors: np.ndarray, save: bool = False, ): ... plot_dynamic_correlation_heatmap Show source in bench_plots.py:1269 Plot the correlation between predictive uncertainty and prediction errors using a heatmap. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. preds_std np.ndarray - Standard deviation of predictions from the ensemble of models. errors np.ndarray - Prediction errors. average_correlation float - The average correlation between gradients and prediction errors (pearson correlation). save bool, optional - Whether to save the plot as a file. threshold_factor float, optional - Fraction of max value below which cells are set to 0. Default is 5e-5. cutoff_percent float, optional - The percentage of total counts to include in the heatmap. Default is 0.95. Signature def plot_dynamic_correlation_heatmap( surr_name: str, conf: dict, preds_std: np.ndarray, errors: np.ndarray, average_correlation: float, save: bool = False, threshold_factor: float = 0.0001, xcut_percent: float = 0.003, ) -> None: ... plot_error_correlation_heatmap Show source in bench_plots.py:1181 Plot the correlation between predictive uncertainty and prediction errors using a heatmap. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. preds_std np.ndarray - Standard deviation of predictions from the ensemble of models. errors np.ndarray - Prediction errors. average_correlation float - The average correlation between gradients and prediction errors (pearson correlation). save bool, optional - Whether to save the plot as a file. threshold_factor float, optional - Fraction of max value below which cells are set to 0. Default is 0.001. Signature def plot_error_correlation_heatmap( surr_name: str, conf: dict, preds_std: np.ndarray, errors: np.ndarray, average_correlation: float, save: bool = False, threshold_factor: float = 0.01, ) -> None: ... plot_error_distribution_comparative Show source in bench_plots.py:1354 Plot the comparative distribution of errors for each surrogate model as a smoothed histogram plot. Arguments conf dict - The configuration dictionary. errors dict - Dictionary containing numpy arrays of shape [num_samples, num_timesteps, num_chemicals] for each model. save bool, optional - Whether to save the plot as a file. Signature def plot_error_distribution_comparative( errors: dict[str, np.ndarray], conf: dict, save: bool = True ) -> None: ... plot_error_distribution_per_chemical Show source in bench_plots.py:655 Plot the distribution of errors for each chemical as a smoothed histogram plot. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. errors np.ndarray - Errors array of shape [num_samples, num_timesteps, num_chemicals]. chemical_names list, optional - List of chemical names for labeling the lines. num_chemicals int, optional - Number of chemicals to plot. Default is 10. save bool, optional - Whether to save the plot as a file. Signature def plot_error_distribution_per_chemical( surr_name: str, conf: dict, errors: np.ndarray, chemical_names: list[str] | None = None, num_chemicals: int = 10, save: bool = True, ) -> None: ... plot_example_predictions_with_uncertainty Show source in bench_plots.py:331 Plot example predictions with uncertainty. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. preds_mean np.ndarray - Mean predictions from the ensemble of models. preds_std np.ndarray - Standard deviation of predictions from the ensemble of models. targets np.ndarray - True targets. timesteps np.ndarray - Timesteps array. example_idx int, optional - Index of the example to plot. Default is 0. num_chemicals int, optional - Number of chemicals to plot. Default is 100. labels list, optional - List of labels for the chemicals. save bool, optional - Whether to save the plot as a file. Signature def plot_example_predictions_with_uncertainty( surr_name: str, conf: dict, preds_mean: np.ndarray, preds_std: np.ndarray, targets: np.ndarray, timesteps: np.ndarray, example_idx: int = 0, num_chemicals: int = 100, labels: list[str] | None = None, save: bool = False, ) -> None: ... plot_generalization_error_comparison Show source in bench_plots.py:1123 Plot the generalization errors of different surrogate models. Arguments surrogates list - List of surrogate model names. metrics_list list[np.array] - List of numpy arrays containing the metrics for each surrogate model. model_errors_list list[np.array] - List of numpy arrays containing the errors for each surrogate model. xlabel str - Label for the x-axis. filename str - Filename to save the plot. config dict - Configuration dictionary. save bool - Whether to save the plot. xlog bool - Whether to use a log scale for the x-axis. Returns None Signature def plot_generalization_error_comparison( surrogates: list[str], metrics_list: list[np.array], model_errors_list: list[np.array], xlabel: str, filename: str, config: dict, save: bool = True, xlog: bool = False, ) -> None: ... plot_generalization_errors Show source in bench_plots.py:198 Plot the generalization errors of a model for various metrics. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. metrics np.ndarray - The metrics (e.g., intervals, cutoffs, batch sizes, number of training samples). model_errors np.ndarray - The model errors. mode str - The mode of generalization (\"interpolation\", \"extrapolation\", \"sparse\", \"batchsize\"). save bool - Whether to save the plot. Returns None Signature def plot_generalization_errors( surr_name: str, conf: dict, metrics: np.ndarray, model_errors: np.ndarray, mode: str, save: bool = False, ) -> None: ... plot_loss_comparison Show source in bench_plots.py:828 Plot the training and test losses for different surrogate models. Arguments train_losses tuple - Tuple of training loss arrays for each surrogate model. test_losses tuple - Tuple of test loss arrays for each surrogate model. labels tuple - Tuple of labels for each surrogate model. config dict - Configuration dictionary. save bool - Whether to save the plot. Returns None Signature def plot_loss_comparison( train_losses: tuple[np.ndarray, ...], test_losses: tuple[np.ndarray, ...], labels: tuple[str, ...], config: dict, save: bool = True, ) -> None: ... plot_losses Show source in bench_plots.py:775 Plot the loss trajectories for the training of multiple models. Arguments loss_histories - List of loss history arrays. labels - List of labels for each loss history. title - Title of the plot. save - Whether to save the plot as an image file. conf - The configuration dictionary. surr_name - The name of the surrogate model. mode - The mode of the training. Signature def plot_losses( loss_histories: tuple[np.array, ...], labels: tuple[str, ...], title: str = \"Losses\", save: bool = False, conf: Optional[dict] = None, surr_name: Optional[str] = None, mode: str = \"main\", ) -> None: ... plot_relative_errors Show source in bench_plots.py:949 Plot the relative errors over time for different surrogate models. Arguments mean_errors dict - dictionary containing the mean relative errors for each surrogate model. median_errors dict - dictionary containing the median relative errors for each surrogate model. timesteps np.ndarray - Array of timesteps. config dict - Configuration dictionary. save bool - Whether to save the plot. Returns None Signature def plot_relative_errors( mean_errors: dict[str, np.ndarray], median_errors: dict[str, np.ndarray], timesteps: np.ndarray, config: dict, save: bool = True, ) -> None: ... plot_relative_errors_over_time Show source in bench_plots.py:86 Plot the mean and median relative errors over time with shaded regions for the 50th, 90th, and 99th percentiles. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. relative_errors np.ndarray - The relative errors of the model. title str - The title of the plot. save bool - Whether to save the plot. Signature def plot_relative_errors_over_time( surr_name: str, conf: dict, relative_errors: np.ndarray, title: str, save: bool = False, ) -> None: ... plot_surr_losses Show source in bench_plots.py:525 Plot the training and test losses for the surrogate model. Arguments model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. timesteps np.ndarray - The timesteps array. Signature def plot_surr_losses( model, surr_name: str, conf: dict, timesteps: np.ndarray ) -> None: ... plot_uncertainty_over_time_comparison Show source in bench_plots.py:1007 Plot the uncertainty over time for different surrogate models. Arguments uncertainties dict - Dictionary containing the uncertainties for each surrogate model. absolute_errors dict - Dictionary containing the absolute errors for each surrogate model. timesteps np.ndarray - Array of timesteps. config dict - Configuration dictionary. save bool - Whether to save the plot. Returns None Signature def plot_uncertainty_over_time_comparison( uncertainties: dict[str, np.ndarray], absolute_errors: dict[str, np.ndarray], timesteps: np.ndarray, config: dict, save: bool = True, ) -> None: ... plot_uncertainty_vs_errors Show source in bench_plots.py:493 Plot the correlation between predictive uncertainty and prediction errors. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. preds_std np.ndarray - Standard deviation of predictions from the ensemble of models. errors np.ndarray - Prediction errors. save bool, optional - Whether to save the plot as a file. Signature def plot_uncertainty_vs_errors( surr_name: str, conf: dict, preds_std: np.ndarray, errors: np.ndarray, save: bool = False, ) -> None: ... rel_errors_and_uq Show source in bench_plots.py:1830 Create a figure with two subplots: relative errors over time and uncertainty over time for different surrogate models. Arguments metrics dict - Dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. save bool - Whether to save the plot. Returns None Signature def rel_errors_and_uq( metrics: dict[str, dict], config: dict, save: bool = True ) -> None: ... save_plot Show source in bench_plots.py:15 Save the plot to a file, creating necessary directories if they don't exist. Arguments plt matplotlib.pyplot - The plot object to save. filename str - The desired filename for the plot. conf dict - The configuration dictionary. surr_name str - The name of the surrogate model. dpi int - The resolution of the saved plot. base_dir str, optional - The base directory where plots will be saved. Default is \"plots\". increase_count bool, optional - Whether to increment the filename count if a file already exists. Default is True. Raises ValueError - If the configuration dictionary does not contain the required keys. Signature def save_plot( plt, filename: str, conf: dict, surr_name: str = \"\", dpi: int = 300, base_dir: str = \"plots\", increase_count: bool = False, ) -> None: ... save_plot_counter Show source in bench_plots.py:54 Save a plot with an incremented filename if a file with the same name already exists. Arguments filename str - The desired filename for the plot. directory str - The directory to save the plot in. increase_count bool, optional - Whether to increment the filename count if a file already exists. Default is True. Returns str - The full path to the saved plot. Signature def save_plot_counter( filename: str, directory: str, increase_count: bool = True ) -> str: ...","title":"Bench Plots"},{"location":"technical/codes/benchmark/bench_plots/#bench-plots","text":"Codes-benchmark Index / Codes / Benchmark / Bench Plots Auto-generated documentation for codes.benchmark.bench_plots module. Bench Plots get_custom_palette inference_time_bar_plot int_ext_sparse plot_MAE_comparison plot_MAE_comparison_train_duration plot_average_errors_over_time plot_average_uncertainty_over_time plot_comparative_dynamic_correlation_heatmaps plot_comparative_error_correlation_heatmaps plot_dynamic_correlation plot_dynamic_correlation_heatmap plot_error_correlation_heatmap plot_error_distribution_comparative plot_error_distribution_per_chemical plot_example_predictions_with_uncertainty plot_generalization_error_comparison plot_generalization_errors plot_loss_comparison plot_losses plot_relative_errors plot_relative_errors_over_time plot_surr_losses plot_uncertainty_over_time_comparison plot_uncertainty_vs_errors rel_errors_and_uq save_plot save_plot_counter","title":"Bench Plots"},{"location":"technical/codes/benchmark/bench_plots/#get_custom_palette","text":"Show source in bench_plots.py:1645 Returns a list of colors sampled from a custom color palette.","title":"get_custom_palette"},{"location":"technical/codes/benchmark/bench_plots/#arguments","text":"num_colors int - The number of colors needed.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns","text":"list - A list of RGBA color tuples.","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature","text":"def get_custom_palette(num_colors): ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#inference_time_bar_plot","text":"Show source in bench_plots.py:1065 Plot the mean inference time with standard deviation for different surrogate models.","title":"inference_time_bar_plot"},{"location":"technical/codes/benchmark/bench_plots/#arguments_1","text":"surrogates List[str] - List of surrogate model names. means List[float] - List of mean inference times for each surrogate model. stds List[float] - List of standard deviation of inference times for each surrogate model. config dict - Configuration dictionary. save bool, optional - Whether to save the plot. Defaults to True.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_1","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_1","text":"def inference_time_bar_plot( surrogates: list[str], means: list[float], stds: list[float], config: dict, save: bool = True, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#int_ext_sparse","text":"Show source in bench_plots.py:1691 Function to make one comparative plot of the interpolation, extrapolation, and sparse training errors.","title":"int_ext_sparse"},{"location":"technical/codes/benchmark/bench_plots/#arguments_2","text":"all_metrics dict - dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_2","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_2","text":"def int_ext_sparse(all_metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_mae_comparison","text":"Show source in bench_plots.py:873 Plot the MAE for different surrogate models.","title":"plot_MAE_comparison"},{"location":"technical/codes/benchmark/bench_plots/#arguments_3","text":"MAE tuple - Tuple of accuracy arrays for each surrogate model. labels tuple - Tuple of labels for each surrogate model. config dict - Configuration dictionary. save bool - Whether to save the plot.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_3","text":"def plot_MAE_comparison( MAEs: tuple[np.ndarray, ...], labels: tuple[str, ...], config: dict, save: bool = True, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_mae_comparison_train_duration","text":"Show source in bench_plots.py:910 Plot the MAE for different surrogate models.","title":"plot_MAE_comparison_train_duration"},{"location":"technical/codes/benchmark/bench_plots/#arguments_4","text":"MAE tuple - Tuple of accuracy arrays for each surrogate model. labels tuple - Tuple of labels for each surrogate model. config dict - Configuration dictionary. save bool - Whether to save the plot.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_4","text":"def plot_MAE_comparison_train_duration( MAEs: tuple[np.ndarray, ...], labels: tuple[str, ...], train_durations: tuple[float, ...], config: dict, save: bool = True, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_average_errors_over_time","text":"Show source in bench_plots.py:258 Plot the errors over time for different modes (interpolation, extrapolation, sparse, batchsize).","title":"plot_average_errors_over_time"},{"location":"technical/codes/benchmark/bench_plots/#arguments_5","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. errors np.ndarray - Errors array of shape [N_metrics, n_timesteps]. metrics np.ndarray - Metrics array of shape [N_metrics]. timesteps np.ndarray - Timesteps array. mode str - The mode of evaluation ('interpolation', 'extrapolation', 'sparse', 'batchsize'). save bool, optional - Whether to save the plot as a file.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_5","text":"def plot_average_errors_over_time( surr_name: str, conf: dict, errors: np.ndarray, metrics: np.ndarray, timesteps: np.ndarray, mode: str, save: bool = False, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_average_uncertainty_over_time","text":"Show source in bench_plots.py:455 Plot the average uncertainty over time.","title":"plot_average_uncertainty_over_time"},{"location":"technical/codes/benchmark/bench_plots/#arguments_6","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. errors_time np.ndarray - Prediction errors over time. preds_std np.ndarray - Standard deviation over time of predictions from the ensemble of models. timesteps np.ndarray - Timesteps array. save bool, optional - Whether to save the plot as a file.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_6","text":"def plot_average_uncertainty_over_time( surr_name: str, conf: dict, errors_time: np.ndarray, preds_std: np.ndarray, timesteps: np.ndarray, save: bool = False, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_comparative_dynamic_correlation_heatmaps","text":"Show source in bench_plots.py:1550 Plot comparative heatmaps of correlation between gradient and prediction errors for multiple surrogate models.","title":"plot_comparative_dynamic_correlation_heatmaps"},{"location":"technical/codes/benchmark/bench_plots/#arguments_7","text":"gradients (dict[str, np.ndarray]): Dictionary of gradients from the ensemble of models. errors (dict[str, np.ndarray]): Dictionary of prediction errors. avg_correlations (dict[str, float]): Dictionary of average correlations between gradients and prediction errors. max_grad (dict[str, float]): Dictionary of maximum gradient values for axis scaling across models. max_err (dict[str, float]): Dictionary of maximum error values for axis scaling across models. max_count (dict[str, float]): Dictionary of maximum count values for heatmap normalization across models. - config dict - Configuration dictionary. - save bool, optional - Whether to save the plot. Defaults to True.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_3","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_7","text":"def plot_comparative_dynamic_correlation_heatmaps( gradients: dict[str, np.ndarray], errors: dict[str, np.ndarray], avg_correlations: dict[str, float], max_grad: dict[str, float], max_err: dict[str, float], max_count: dict[str, float], config: dict, save: bool = True, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_comparative_error_correlation_heatmaps","text":"Show source in bench_plots.py:1453 Plot comparative heatmaps of correlation between predictive uncertainty and prediction errors for multiple surrogate models.","title":"plot_comparative_error_correlation_heatmaps"},{"location":"technical/codes/benchmark/bench_plots/#arguments_8","text":"preds_std (dict[str, np.ndarray]): Dictionary of standard deviation of predictions from the ensemble of models. errors (dict[str, np.ndarray]): Dictionary of prediction errors. avg_correlations (dict[str, float]): Dictionary of average correlations between gradients and prediction errors. axis_max (dict[str, float]): Dictionary of maximum values for axis scaling across models. max_count (dict[str, float]): Dictionary of maximum count values for heatmap normalization across models. - config dict - Configuration dictionary. - save bool, optional - Whether to save the plot. Defaults to True.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_4","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_8","text":"def plot_comparative_error_correlation_heatmaps( preds_std: dict[str, np.ndarray], errors: dict[str, np.ndarray], avg_correlations: dict[str, float], axis_max: dict[str, float], max_count: dict[str, float], config: dict, save: bool = True, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_dynamic_correlation","text":"Show source in bench_plots.py:163 Plot the correlation between the gradients of the data and the prediction errors.","title":"plot_dynamic_correlation"},{"location":"technical/codes/benchmark/bench_plots/#arguments_9","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. gradients np.ndarray - The gradients of the data. errors np.ndarray - The prediction errors. save bool - Whether to save the plot.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_9","text":"def plot_dynamic_correlation( surr_name: str, conf: dict, gradients: np.ndarray, errors: np.ndarray, save: bool = False, ): ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_dynamic_correlation_heatmap","text":"Show source in bench_plots.py:1269 Plot the correlation between predictive uncertainty and prediction errors using a heatmap.","title":"plot_dynamic_correlation_heatmap"},{"location":"technical/codes/benchmark/bench_plots/#arguments_10","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. preds_std np.ndarray - Standard deviation of predictions from the ensemble of models. errors np.ndarray - Prediction errors. average_correlation float - The average correlation between gradients and prediction errors (pearson correlation). save bool, optional - Whether to save the plot as a file. threshold_factor float, optional - Fraction of max value below which cells are set to 0. Default is 5e-5. cutoff_percent float, optional - The percentage of total counts to include in the heatmap. Default is 0.95.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_10","text":"def plot_dynamic_correlation_heatmap( surr_name: str, conf: dict, preds_std: np.ndarray, errors: np.ndarray, average_correlation: float, save: bool = False, threshold_factor: float = 0.0001, xcut_percent: float = 0.003, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_error_correlation_heatmap","text":"Show source in bench_plots.py:1181 Plot the correlation between predictive uncertainty and prediction errors using a heatmap.","title":"plot_error_correlation_heatmap"},{"location":"technical/codes/benchmark/bench_plots/#arguments_11","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. preds_std np.ndarray - Standard deviation of predictions from the ensemble of models. errors np.ndarray - Prediction errors. average_correlation float - The average correlation between gradients and prediction errors (pearson correlation). save bool, optional - Whether to save the plot as a file. threshold_factor float, optional - Fraction of max value below which cells are set to 0. Default is 0.001.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_11","text":"def plot_error_correlation_heatmap( surr_name: str, conf: dict, preds_std: np.ndarray, errors: np.ndarray, average_correlation: float, save: bool = False, threshold_factor: float = 0.01, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_error_distribution_comparative","text":"Show source in bench_plots.py:1354 Plot the comparative distribution of errors for each surrogate model as a smoothed histogram plot.","title":"plot_error_distribution_comparative"},{"location":"technical/codes/benchmark/bench_plots/#arguments_12","text":"conf dict - The configuration dictionary. errors dict - Dictionary containing numpy arrays of shape [num_samples, num_timesteps, num_chemicals] for each model. save bool, optional - Whether to save the plot as a file.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_12","text":"def plot_error_distribution_comparative( errors: dict[str, np.ndarray], conf: dict, save: bool = True ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_error_distribution_per_chemical","text":"Show source in bench_plots.py:655 Plot the distribution of errors for each chemical as a smoothed histogram plot.","title":"plot_error_distribution_per_chemical"},{"location":"technical/codes/benchmark/bench_plots/#arguments_13","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. errors np.ndarray - Errors array of shape [num_samples, num_timesteps, num_chemicals]. chemical_names list, optional - List of chemical names for labeling the lines. num_chemicals int, optional - Number of chemicals to plot. Default is 10. save bool, optional - Whether to save the plot as a file.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_13","text":"def plot_error_distribution_per_chemical( surr_name: str, conf: dict, errors: np.ndarray, chemical_names: list[str] | None = None, num_chemicals: int = 10, save: bool = True, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_example_predictions_with_uncertainty","text":"Show source in bench_plots.py:331 Plot example predictions with uncertainty.","title":"plot_example_predictions_with_uncertainty"},{"location":"technical/codes/benchmark/bench_plots/#arguments_14","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. preds_mean np.ndarray - Mean predictions from the ensemble of models. preds_std np.ndarray - Standard deviation of predictions from the ensemble of models. targets np.ndarray - True targets. timesteps np.ndarray - Timesteps array. example_idx int, optional - Index of the example to plot. Default is 0. num_chemicals int, optional - Number of chemicals to plot. Default is 100. labels list, optional - List of labels for the chemicals. save bool, optional - Whether to save the plot as a file.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_14","text":"def plot_example_predictions_with_uncertainty( surr_name: str, conf: dict, preds_mean: np.ndarray, preds_std: np.ndarray, targets: np.ndarray, timesteps: np.ndarray, example_idx: int = 0, num_chemicals: int = 100, labels: list[str] | None = None, save: bool = False, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_generalization_error_comparison","text":"Show source in bench_plots.py:1123 Plot the generalization errors of different surrogate models.","title":"plot_generalization_error_comparison"},{"location":"technical/codes/benchmark/bench_plots/#arguments_15","text":"surrogates list - List of surrogate model names. metrics_list list[np.array] - List of numpy arrays containing the metrics for each surrogate model. model_errors_list list[np.array] - List of numpy arrays containing the errors for each surrogate model. xlabel str - Label for the x-axis. filename str - Filename to save the plot. config dict - Configuration dictionary. save bool - Whether to save the plot. xlog bool - Whether to use a log scale for the x-axis.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_5","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_15","text":"def plot_generalization_error_comparison( surrogates: list[str], metrics_list: list[np.array], model_errors_list: list[np.array], xlabel: str, filename: str, config: dict, save: bool = True, xlog: bool = False, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_generalization_errors","text":"Show source in bench_plots.py:198 Plot the generalization errors of a model for various metrics.","title":"plot_generalization_errors"},{"location":"technical/codes/benchmark/bench_plots/#arguments_16","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. metrics np.ndarray - The metrics (e.g., intervals, cutoffs, batch sizes, number of training samples). model_errors np.ndarray - The model errors. mode str - The mode of generalization (\"interpolation\", \"extrapolation\", \"sparse\", \"batchsize\"). save bool - Whether to save the plot.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_6","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_16","text":"def plot_generalization_errors( surr_name: str, conf: dict, metrics: np.ndarray, model_errors: np.ndarray, mode: str, save: bool = False, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_loss_comparison","text":"Show source in bench_plots.py:828 Plot the training and test losses for different surrogate models.","title":"plot_loss_comparison"},{"location":"technical/codes/benchmark/bench_plots/#arguments_17","text":"train_losses tuple - Tuple of training loss arrays for each surrogate model. test_losses tuple - Tuple of test loss arrays for each surrogate model. labels tuple - Tuple of labels for each surrogate model. config dict - Configuration dictionary. save bool - Whether to save the plot.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_7","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_17","text":"def plot_loss_comparison( train_losses: tuple[np.ndarray, ...], test_losses: tuple[np.ndarray, ...], labels: tuple[str, ...], config: dict, save: bool = True, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_losses","text":"Show source in bench_plots.py:775 Plot the loss trajectories for the training of multiple models.","title":"plot_losses"},{"location":"technical/codes/benchmark/bench_plots/#arguments_18","text":"loss_histories - List of loss history arrays. labels - List of labels for each loss history. title - Title of the plot. save - Whether to save the plot as an image file. conf - The configuration dictionary. surr_name - The name of the surrogate model. mode - The mode of the training.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_18","text":"def plot_losses( loss_histories: tuple[np.array, ...], labels: tuple[str, ...], title: str = \"Losses\", save: bool = False, conf: Optional[dict] = None, surr_name: Optional[str] = None, mode: str = \"main\", ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_relative_errors","text":"Show source in bench_plots.py:949 Plot the relative errors over time for different surrogate models.","title":"plot_relative_errors"},{"location":"technical/codes/benchmark/bench_plots/#arguments_19","text":"mean_errors dict - dictionary containing the mean relative errors for each surrogate model. median_errors dict - dictionary containing the median relative errors for each surrogate model. timesteps np.ndarray - Array of timesteps. config dict - Configuration dictionary. save bool - Whether to save the plot.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_8","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_19","text":"def plot_relative_errors( mean_errors: dict[str, np.ndarray], median_errors: dict[str, np.ndarray], timesteps: np.ndarray, config: dict, save: bool = True, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_relative_errors_over_time","text":"Show source in bench_plots.py:86 Plot the mean and median relative errors over time with shaded regions for the 50th, 90th, and 99th percentiles.","title":"plot_relative_errors_over_time"},{"location":"technical/codes/benchmark/bench_plots/#arguments_20","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. relative_errors np.ndarray - The relative errors of the model. title str - The title of the plot. save bool - Whether to save the plot.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_20","text":"def plot_relative_errors_over_time( surr_name: str, conf: dict, relative_errors: np.ndarray, title: str, save: bool = False, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_surr_losses","text":"Show source in bench_plots.py:525 Plot the training and test losses for the surrogate model.","title":"plot_surr_losses"},{"location":"technical/codes/benchmark/bench_plots/#arguments_21","text":"model - Instance of the surrogate model class. surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. timesteps np.ndarray - The timesteps array.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_21","text":"def plot_surr_losses( model, surr_name: str, conf: dict, timesteps: np.ndarray ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_uncertainty_over_time_comparison","text":"Show source in bench_plots.py:1007 Plot the uncertainty over time for different surrogate models.","title":"plot_uncertainty_over_time_comparison"},{"location":"technical/codes/benchmark/bench_plots/#arguments_22","text":"uncertainties dict - Dictionary containing the uncertainties for each surrogate model. absolute_errors dict - Dictionary containing the absolute errors for each surrogate model. timesteps np.ndarray - Array of timesteps. config dict - Configuration dictionary. save bool - Whether to save the plot.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_9","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_22","text":"def plot_uncertainty_over_time_comparison( uncertainties: dict[str, np.ndarray], absolute_errors: dict[str, np.ndarray], timesteps: np.ndarray, config: dict, save: bool = True, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#plot_uncertainty_vs_errors","text":"Show source in bench_plots.py:493 Plot the correlation between predictive uncertainty and prediction errors.","title":"plot_uncertainty_vs_errors"},{"location":"technical/codes/benchmark/bench_plots/#arguments_23","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. preds_std np.ndarray - Standard deviation of predictions from the ensemble of models. errors np.ndarray - Prediction errors. save bool, optional - Whether to save the plot as a file.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#signature_23","text":"def plot_uncertainty_vs_errors( surr_name: str, conf: dict, preds_std: np.ndarray, errors: np.ndarray, save: bool = False, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#rel_errors_and_uq","text":"Show source in bench_plots.py:1830 Create a figure with two subplots: relative errors over time and uncertainty over time for different surrogate models.","title":"rel_errors_and_uq"},{"location":"technical/codes/benchmark/bench_plots/#arguments_24","text":"metrics dict - Dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. save bool - Whether to save the plot.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_10","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_24","text":"def rel_errors_and_uq( metrics: dict[str, dict], config: dict, save: bool = True ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#save_plot","text":"Show source in bench_plots.py:15 Save the plot to a file, creating necessary directories if they don't exist.","title":"save_plot"},{"location":"technical/codes/benchmark/bench_plots/#arguments_25","text":"plt matplotlib.pyplot - The plot object to save. filename str - The desired filename for the plot. conf dict - The configuration dictionary. surr_name str - The name of the surrogate model. dpi int - The resolution of the saved plot. base_dir str, optional - The base directory where plots will be saved. Default is \"plots\". increase_count bool, optional - Whether to increment the filename count if a file already exists. Default is True.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#raises","text":"ValueError - If the configuration dictionary does not contain the required keys.","title":"Raises"},{"location":"technical/codes/benchmark/bench_plots/#signature_25","text":"def save_plot( plt, filename: str, conf: dict, surr_name: str = \"\", dpi: int = 300, base_dir: str = \"plots\", increase_count: bool = False, ) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_plots/#save_plot_counter","text":"Show source in bench_plots.py:54 Save a plot with an incremented filename if a file with the same name already exists.","title":"save_plot_counter"},{"location":"technical/codes/benchmark/bench_plots/#arguments_26","text":"filename str - The desired filename for the plot. directory str - The directory to save the plot in. increase_count bool, optional - Whether to increment the filename count if a file already exists. Default is True.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_plots/#returns_11","text":"str - The full path to the saved plot.","title":"Returns"},{"location":"technical/codes/benchmark/bench_plots/#signature_26","text":"def save_plot_counter( filename: str, directory: str, increase_count: bool = True ) -> str: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/","text":"Bench Utils Codes-benchmark Index / Codes / Benchmark / Bench Utils Auto-generated documentation for codes.benchmark.bench_utils module. Bench Utils check_benchmark check_surrogate clean_metrics convert_dict_to_scientific_notation convert_to_standard_types count_trainable_parameters discard_numpy_entries flatten_dict format_seconds format_time get_model_config get_required_models_list get_surrogate load_model make_comparison_csv measure_memory_footprint read_yaml_config write_metrics_to_yaml check_benchmark Show source in bench_utils.py:43 Check whether there are any configuration issues with the benchmark. Arguments conf dict - The configuration dictionary. Raises FileNotFoundError - If the training ID directory is missing or if the .yaml file is missing. ValueError - If the configuration is missing required keys or the values do not match the training configuration. Signature def check_benchmark(conf: dict) -> None: ... check_surrogate Show source in bench_utils.py:16 Check whether the required models for the benchmark are present in the expected directories. Arguments surrogate str - The name of the surrogate model to check. conf dict - The configuration dictionary. Raises FileNotFoundError - If any required models are missing. Signature def check_surrogate(surrogate: str, conf: dict) -> None: ... clean_metrics Show source in bench_utils.py:392 Clean the metrics dictionary to remove problematic entries. Arguments metrics dict - The benchmark metrics. conf dict - The configuration dictionary. Returns dict - The cleaned metrics dictionary. Signature def clean_metrics(metrics: dict, conf: dict) -> dict: ... convert_dict_to_scientific_notation Show source in bench_utils.py:542 Convert all numerical values in a dictionary to scientific notation. Arguments d dict - The input dictionary. Returns dict - The dictionary with numerical values in scientific notation. Signature def convert_dict_to_scientific_notation(d: dict, precision: int = 8) -> dict: ... convert_to_standard_types Show source in bench_utils.py:343 Recursively convert data to standard types that can be serialized to YAML. Arguments data - The data to convert. Returns The converted data. Signature def convert_to_standard_types(data): ... count_trainable_parameters Show source in bench_utils.py:264 Count the number of trainable parameters in the model. Arguments model torch.nn.Module - The PyTorch model. Returns int - The number of trainable parameters. Signature def count_trainable_parameters(model: torch.nn.Module) -> int: ... discard_numpy_entries Show source in bench_utils.py:367 Recursively remove dictionary entries that contain NumPy arrays. Arguments d dict - The input dictionary. Returns dict - A new dictionary without entries containing NumPy arrays. Signature def discard_numpy_entries(d: dict) -> dict: ... flatten_dict Show source in bench_utils.py:520 Flatten a nested dictionary. Arguments d dict - The dictionary to flatten. parent_key str - The base key string. sep str - The separator between keys. Returns dict - Flattened dictionary with composite keys. Signature def flatten_dict(d: dict, parent_key: str = \"\", sep: str = \" - \") -> dict: ... format_seconds Show source in bench_utils.py:504 Format a duration given in seconds as hh:mm:ss. Arguments seconds int - The duration in seconds. Returns str - The formatted duration string. Signature def format_seconds(seconds: int) -> str: ... format_time Show source in bench_utils.py:479 Format mean and std time consistently in ns, \u00b5s, ms, or s. Arguments mean_time - The mean time. std_time - The standard deviation of the time. Returns str - The formatted time string. Signature def format_time(mean_time, std_time): ... get_model_config Show source in bench_utils.py:621 Get the model configuration for a specific surrogate model from the dataset folder. Returns an empty dictionary if the configuration file is not found. Arguments surr_name str - The name of the surrogate model. config dict - The configuration dictionary. Returns dict - The model configuration dictionary. Signature def get_model_config(surr_name: str, config: dict) -> dict: ... get_required_models_list Show source in bench_utils.py:178 Generate a list of required models based on the configuration settings. Arguments surrogate str - The name of the surrogate model. conf dict - The configuration dictionary. Returns list - A list of required model names. Signature def get_required_models_list(surrogate: str, conf: dict) -> list: ... get_surrogate Show source in bench_utils.py:462 Check if the surrogate model exists. Arguments surrogate_name str - The name of the surrogate model. Returns SurrogateModel | None: The surrogate model class if it exists, otherwise None. Signature def get_surrogate(surrogate_name: str) -> SurrogateModel | None: ... load_model Show source in bench_utils.py:241 Load a trained surrogate model. Arguments model - Instance of the surrogate model class. training_id str - The training identifier. surr_name str - The name of the surrogate model. model_identifier str - The identifier of the model (e.g., 'main'). Returns The loaded surrogate model. Signature def load_model( model, training_id: str, surr_name: str, model_identifier: str ) -> torch.nn.Module: ... make_comparison_csv Show source in bench_utils.py:558 Generate a CSV file comparing metrics for different surrogate models. Arguments metrics dict - Dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary. Returns None Signature def make_comparison_csv(metrics: dict, config: dict) -> None: ... measure_memory_footprint Show source in bench_utils.py:277 Measure the memory footprint of the model during the forward and backward pass. Arguments model torch.nn.Module - The PyTorch model. inputs tuple - The input data for the model. conf dict - The configuration dictionary. surr_name str - The name of the surrogate model. Returns dict - A dictionary containing memory footprint measurements. Signature def measure_memory_footprint(model: torch.nn.Module, inputs: tuple) -> dict: ... read_yaml_config Show source in bench_utils.py:226 Read the YAML configuration file. Arguments config_path str - Path to the YAML configuration file. Returns dict - The configuration dictionary. Signature def read_yaml_config(config_path: str) -> dict: ... write_metrics_to_yaml Show source in bench_utils.py:433 Write the benchmark metrics to a YAML file. Arguments surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. metrics dict - The benchmark metrics. Signature def write_metrics_to_yaml(surr_name: str, conf: dict, metrics: dict) -> None: ...","title":"Bench Utils"},{"location":"technical/codes/benchmark/bench_utils/#bench-utils","text":"Codes-benchmark Index / Codes / Benchmark / Bench Utils Auto-generated documentation for codes.benchmark.bench_utils module. Bench Utils check_benchmark check_surrogate clean_metrics convert_dict_to_scientific_notation convert_to_standard_types count_trainable_parameters discard_numpy_entries flatten_dict format_seconds format_time get_model_config get_required_models_list get_surrogate load_model make_comparison_csv measure_memory_footprint read_yaml_config write_metrics_to_yaml","title":"Bench Utils"},{"location":"technical/codes/benchmark/bench_utils/#check_benchmark","text":"Show source in bench_utils.py:43 Check whether there are any configuration issues with the benchmark.","title":"check_benchmark"},{"location":"technical/codes/benchmark/bench_utils/#arguments","text":"conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#raises","text":"FileNotFoundError - If the training ID directory is missing or if the .yaml file is missing. ValueError - If the configuration is missing required keys or the values do not match the training configuration.","title":"Raises"},{"location":"technical/codes/benchmark/bench_utils/#signature","text":"def check_benchmark(conf: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#check_surrogate","text":"Show source in bench_utils.py:16 Check whether the required models for the benchmark are present in the expected directories.","title":"check_surrogate"},{"location":"technical/codes/benchmark/bench_utils/#arguments_1","text":"surrogate str - The name of the surrogate model to check. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#raises_1","text":"FileNotFoundError - If any required models are missing.","title":"Raises"},{"location":"technical/codes/benchmark/bench_utils/#signature_1","text":"def check_surrogate(surrogate: str, conf: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#clean_metrics","text":"Show source in bench_utils.py:392 Clean the metrics dictionary to remove problematic entries.","title":"clean_metrics"},{"location":"technical/codes/benchmark/bench_utils/#arguments_2","text":"metrics dict - The benchmark metrics. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns","text":"dict - The cleaned metrics dictionary.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_2","text":"def clean_metrics(metrics: dict, conf: dict) -> dict: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#convert_dict_to_scientific_notation","text":"Show source in bench_utils.py:542 Convert all numerical values in a dictionary to scientific notation.","title":"convert_dict_to_scientific_notation"},{"location":"technical/codes/benchmark/bench_utils/#arguments_3","text":"d dict - The input dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_1","text":"dict - The dictionary with numerical values in scientific notation.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_3","text":"def convert_dict_to_scientific_notation(d: dict, precision: int = 8) -> dict: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#convert_to_standard_types","text":"Show source in bench_utils.py:343 Recursively convert data to standard types that can be serialized to YAML.","title":"convert_to_standard_types"},{"location":"technical/codes/benchmark/bench_utils/#arguments_4","text":"data - The data to convert.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_2","text":"The converted data.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_4","text":"def convert_to_standard_types(data): ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#count_trainable_parameters","text":"Show source in bench_utils.py:264 Count the number of trainable parameters in the model.","title":"count_trainable_parameters"},{"location":"technical/codes/benchmark/bench_utils/#arguments_5","text":"model torch.nn.Module - The PyTorch model.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_3","text":"int - The number of trainable parameters.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_5","text":"def count_trainable_parameters(model: torch.nn.Module) -> int: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#discard_numpy_entries","text":"Show source in bench_utils.py:367 Recursively remove dictionary entries that contain NumPy arrays.","title":"discard_numpy_entries"},{"location":"technical/codes/benchmark/bench_utils/#arguments_6","text":"d dict - The input dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_4","text":"dict - A new dictionary without entries containing NumPy arrays.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_6","text":"def discard_numpy_entries(d: dict) -> dict: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#flatten_dict","text":"Show source in bench_utils.py:520 Flatten a nested dictionary.","title":"flatten_dict"},{"location":"technical/codes/benchmark/bench_utils/#arguments_7","text":"d dict - The dictionary to flatten. parent_key str - The base key string. sep str - The separator between keys.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_5","text":"dict - Flattened dictionary with composite keys.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_7","text":"def flatten_dict(d: dict, parent_key: str = \"\", sep: str = \" - \") -> dict: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#format_seconds","text":"Show source in bench_utils.py:504 Format a duration given in seconds as hh:mm:ss.","title":"format_seconds"},{"location":"technical/codes/benchmark/bench_utils/#arguments_8","text":"seconds int - The duration in seconds.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_6","text":"str - The formatted duration string.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_8","text":"def format_seconds(seconds: int) -> str: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#format_time","text":"Show source in bench_utils.py:479 Format mean and std time consistently in ns, \u00b5s, ms, or s.","title":"format_time"},{"location":"technical/codes/benchmark/bench_utils/#arguments_9","text":"mean_time - The mean time. std_time - The standard deviation of the time.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_7","text":"str - The formatted time string.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_9","text":"def format_time(mean_time, std_time): ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#get_model_config","text":"Show source in bench_utils.py:621 Get the model configuration for a specific surrogate model from the dataset folder. Returns an empty dictionary if the configuration file is not found.","title":"get_model_config"},{"location":"technical/codes/benchmark/bench_utils/#arguments_10","text":"surr_name str - The name of the surrogate model. config dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_8","text":"dict - The model configuration dictionary.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_10","text":"def get_model_config(surr_name: str, config: dict) -> dict: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#get_required_models_list","text":"Show source in bench_utils.py:178 Generate a list of required models based on the configuration settings.","title":"get_required_models_list"},{"location":"technical/codes/benchmark/bench_utils/#arguments_11","text":"surrogate str - The name of the surrogate model. conf dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_9","text":"list - A list of required model names.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_11","text":"def get_required_models_list(surrogate: str, conf: dict) -> list: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#get_surrogate","text":"Show source in bench_utils.py:462 Check if the surrogate model exists.","title":"get_surrogate"},{"location":"technical/codes/benchmark/bench_utils/#arguments_12","text":"surrogate_name str - The name of the surrogate model.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_10","text":"SurrogateModel | None: The surrogate model class if it exists, otherwise None.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_12","text":"def get_surrogate(surrogate_name: str) -> SurrogateModel | None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#load_model","text":"Show source in bench_utils.py:241 Load a trained surrogate model.","title":"load_model"},{"location":"technical/codes/benchmark/bench_utils/#arguments_13","text":"model - Instance of the surrogate model class. training_id str - The training identifier. surr_name str - The name of the surrogate model. model_identifier str - The identifier of the model (e.g., 'main').","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_11","text":"The loaded surrogate model.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_13","text":"def load_model( model, training_id: str, surr_name: str, model_identifier: str ) -> torch.nn.Module: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#make_comparison_csv","text":"Show source in bench_utils.py:558 Generate a CSV file comparing metrics for different surrogate models.","title":"make_comparison_csv"},{"location":"technical/codes/benchmark/bench_utils/#arguments_14","text":"metrics dict - Dictionary containing the benchmark metrics for each surrogate model. config dict - Configuration dictionary.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_12","text":"None","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_14","text":"def make_comparison_csv(metrics: dict, config: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#measure_memory_footprint","text":"Show source in bench_utils.py:277 Measure the memory footprint of the model during the forward and backward pass.","title":"measure_memory_footprint"},{"location":"technical/codes/benchmark/bench_utils/#arguments_15","text":"model torch.nn.Module - The PyTorch model. inputs tuple - The input data for the model. conf dict - The configuration dictionary. surr_name str - The name of the surrogate model.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_13","text":"dict - A dictionary containing memory footprint measurements.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_15","text":"def measure_memory_footprint(model: torch.nn.Module, inputs: tuple) -> dict: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#read_yaml_config","text":"Show source in bench_utils.py:226 Read the YAML configuration file.","title":"read_yaml_config"},{"location":"technical/codes/benchmark/bench_utils/#arguments_16","text":"config_path str - Path to the YAML configuration file.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#returns_14","text":"dict - The configuration dictionary.","title":"Returns"},{"location":"technical/codes/benchmark/bench_utils/#signature_16","text":"def read_yaml_config(config_path: str) -> dict: ...","title":"Signature"},{"location":"technical/codes/benchmark/bench_utils/#write_metrics_to_yaml","text":"Show source in bench_utils.py:433 Write the benchmark metrics to a YAML file.","title":"write_metrics_to_yaml"},{"location":"technical/codes/benchmark/bench_utils/#arguments_17","text":"surr_name str - The name of the surrogate model. conf dict - The configuration dictionary. metrics dict - The benchmark metrics.","title":"Arguments"},{"location":"technical/codes/benchmark/bench_utils/#signature_17","text":"def write_metrics_to_yaml(surr_name: str, conf: dict, metrics: dict) -> None: ...","title":"Signature"},{"location":"technical/codes/surrogates/","text":"Surrogates Codes-benchmark Index / Codes / Surrogates Auto-generated documentation for codes.surrogates module. Surrogates Modules Modules Surrogate Classes Surrogates","title":"Surrogates"},{"location":"technical/codes/surrogates/#surrogates","text":"Codes-benchmark Index / Codes / Surrogates Auto-generated documentation for codes.surrogates module. Surrogates Modules","title":"Surrogates"},{"location":"technical/codes/surrogates/#modules","text":"Surrogate Classes Surrogates","title":"Modules"},{"location":"technical/codes/surrogates/surrogate_classes/","text":"Surrogate Classes Codes-benchmark Index / Codes / Surrogates / Surrogate Classes Auto-generated documentation for codes.surrogates.surrogate_classes module. - Surrogate Classes","title":"Surrogate Classes"},{"location":"technical/codes/surrogates/surrogate_classes/#surrogate-classes","text":"Codes-benchmark Index / Codes / Surrogates / Surrogate Classes Auto-generated documentation for codes.surrogates.surrogate_classes module. - Surrogate Classes","title":"Surrogate Classes"},{"location":"technical/codes/surrogates/surrogates/","text":"Surrogates Codes-benchmark Index / Codes / Surrogates / Surrogates Auto-generated documentation for codes.surrogates.surrogates module. Surrogates AbstractSurrogateModel AbstractSurrogateModel().denormalize AbstractSurrogateModel().fit AbstractSurrogateModel().forward AbstractSurrogateModel().load AbstractSurrogateModel().predict AbstractSurrogateModel().prepare_data AbstractSurrogateModel().save AbstractSurrogateModel().setup_progress_bar AbstractSurrogateModel Show source in surrogates.py:16 Abstract base class for surrogate models. This class implements the basic structure of a surrogate model and defines the methods that need to be implemented by the subclasses for it to be compatible with the benchmarking framework. For more information, see https://robin-janssen.github.io/CODES-Benchmark/pages/documentation.html#add_model. Arguments device str, optional - The device to run the model on. Defaults to None. n_chemicals int, optional - The number of chemicals. Defaults to 29. n_timesteps int, optional - The number of timesteps. Defaults to 100. config dict, optional - The configuration dictionary. Defaults to {}. Attributes train_loss float - The training loss. test_loss float - The test loss. MAE float - The mean absolute error. normalisation dict - The normalisation parameters. train_duration float - The training duration. device str - The device to run the model on. n_chemicals int - The number of chemicals. n_timesteps int - The number of timesteps. L1 nn.L1Loss - The L1 loss function. config dict - The configuration dictionary. Methods forward(inputs - Any) -> tuple[Tensor, Tensor]: Forward pass of the model. prepare_data( - dataset_train - np.ndarray, - dataset_test - np.ndarray | None, - dataset_val - np.ndarray | None, - timesteps - np.ndarray, - batch_size - int, - shuffle - bool, ) -> tuple[DataLoader, DataLoader, DataLoader]: Gets the data loaders for training, testing, and validation. fit( - train_loader - DataLoader, - test_loader - DataLoader, - epochs - int | None, - position - int, - description - str, ) -> None: Trains the model on the training data. Sets the train_loss and test_loss attributes. predict(data_loader - DataLoader) -> tuple[torch.Tensor, torch.Tensor]: Evaluates the model on the given data loader. save( - model_name - str, - subfolder - str, - training_id - str, - data_params - dict, ) -> None: Saves the model to disk. load(training_id - str, surr_name: str, model_identifier: str) -> None: Loads a trained surrogate model. setup_progress_bar(epochs - int, position: int, description: str) -> tqdm: Helper function to set up a progress bar for training. denormalize(data - torch.Tensor) -> torch.Tensor: Denormalizes the data back to the original scale. Signature class AbstractSurrogateModel(ABC, nn.Module): def __init__( self, device: str | None = None, n_chemicals: int = 29, n_timesteps: int = 100, config: dict | None = None, ): ... AbstractSurrogateModel().denormalize Show source in surrogates.py:362 Denormalize the data. Arguments data np.ndarray - The data to denormalize. Returns np.ndarray - The denormalized data. Signature def denormalize(self, data: torch.Tensor) -> torch.Tensor: ... AbstractSurrogateModel().fit Show source in surrogates.py:147 Perform the training of the model. Sets the train_loss and test_loss attributes. Arguments train_loader DataLoader - The DataLoader object containing the training data. test_loader DataLoader - The DataLoader object containing the testing data. epochs int - The number of epochs to train the model for. position int - The position of the progress bar. description str - The description of the progress bar. Signature @abstractmethod def fit( self, train_loader: DataLoader, test_loader: DataLoader, epochs: int, position: int, description: str, ) -> None: ... AbstractSurrogateModel().forward Show source in surrogates.py:106 Forward pass of the model. Arguments inputs Any - The input data as recieved from the dataloader. Returns tuple[Tensor, Tensor] - The model predictions and the targets. Signature @abstractmethod def forward(self, inputs: Any) -> tuple[Tensor, Tensor]: ... AbstractSurrogateModel().load Show source in surrogates.py:294 Load a trained surrogate model. Arguments training_id str - The training identifier. surr_name str - The name of the surrogate model. model_identifier str - The identifier of the model (e.g., 'main'). Returns None. The model is loaded in place. Signature def load( self, training_id: str, surr_name: str, model_identifier: str, model_dir: str | None = None, ) -> None: ... AbstractSurrogateModel().predict Show source in surrogates.py:168 Evaluate the model on the given dataloader. Arguments data_loader DataLoader - The DataLoader object containing the data the model is evaluated on. Returns tuple[torch.Tensor, torch.Tensor] - The predictions and targets. Signature def predict(self, data_loader: DataLoader) -> tuple[torch.Tensor, torch.Tensor]: ... AbstractSurrogateModel().prepare_data Show source in surrogates.py:119 Prepare the data for training, testing, and validation. This method should return the DataLoader objects for the training, testing, and validation data. Arguments dataset_train np.ndarray - The training dataset. dataset_test np.ndarray - The testing dataset. dataset_val np.ndarray - The validation dataset. timesteps np.ndarray - The timesteps. batch_size int - The batch size. shuffle bool - Whether to shuffle the data. Returns tuple[DataLoader, DataLoader, DataLoader]: The DataLoader objects for the training, testing, and validation data. Signature @abstractmethod def prepare_data( self, dataset_train: np.ndarray, dataset_test: np.ndarray | None, dataset_val: np.ndarray | None, timesteps: np.ndarray, batch_size: int, shuffle: bool, ) -> tuple[DataLoader, DataLoader | None, DataLoader | None]: ... AbstractSurrogateModel().save Show source in surrogates.py:219 Save the model to disk. Arguments model_name str - The name of the model. subfolder str - The subfolder to save the model in. training_id str - The training identifier. data_params dict - The data parameters. Signature def save( self, model_name: str, base_dir: str, training_id: str, data_params: dict ) -> None: ... AbstractSurrogateModel().setup_progress_bar Show source in surrogates.py:337 Helper function to set up a progress bar for training. Arguments epochs int - The number of epochs. position int - The position of the progress bar. description str - The description of the progress bar. Returns tqdm - The progress bar. Signature def setup_progress_bar(self, epochs: int, position: int, description: str): ...","title":"Surrogates"},{"location":"technical/codes/surrogates/surrogates/#surrogates","text":"Codes-benchmark Index / Codes / Surrogates / Surrogates Auto-generated documentation for codes.surrogates.surrogates module. Surrogates AbstractSurrogateModel AbstractSurrogateModel().denormalize AbstractSurrogateModel().fit AbstractSurrogateModel().forward AbstractSurrogateModel().load AbstractSurrogateModel().predict AbstractSurrogateModel().prepare_data AbstractSurrogateModel().save AbstractSurrogateModel().setup_progress_bar","title":"Surrogates"},{"location":"technical/codes/surrogates/surrogates/#abstractsurrogatemodel","text":"Show source in surrogates.py:16 Abstract base class for surrogate models. This class implements the basic structure of a surrogate model and defines the methods that need to be implemented by the subclasses for it to be compatible with the benchmarking framework. For more information, see https://robin-janssen.github.io/CODES-Benchmark/pages/documentation.html#add_model.","title":"AbstractSurrogateModel"},{"location":"technical/codes/surrogates/surrogates/#arguments","text":"device str, optional - The device to run the model on. Defaults to None. n_chemicals int, optional - The number of chemicals. Defaults to 29. n_timesteps int, optional - The number of timesteps. Defaults to 100. config dict, optional - The configuration dictionary. Defaults to {}.","title":"Arguments"},{"location":"technical/codes/surrogates/surrogates/#attributes","text":"train_loss float - The training loss. test_loss float - The test loss. MAE float - The mean absolute error. normalisation dict - The normalisation parameters. train_duration float - The training duration. device str - The device to run the model on. n_chemicals int - The number of chemicals. n_timesteps int - The number of timesteps. L1 nn.L1Loss - The L1 loss function. config dict - The configuration dictionary.","title":"Attributes"},{"location":"technical/codes/surrogates/surrogates/#methods","text":"forward(inputs - Any) -> tuple[Tensor, Tensor]: Forward pass of the model. prepare_data( - dataset_train - np.ndarray, - dataset_test - np.ndarray | None, - dataset_val - np.ndarray | None, - timesteps - np.ndarray, - batch_size - int, - shuffle - bool, ) -> tuple[DataLoader, DataLoader, DataLoader]: Gets the data loaders for training, testing, and validation. fit( - train_loader - DataLoader, - test_loader - DataLoader, - epochs - int | None, - position - int, - description - str, ) -> None: Trains the model on the training data. Sets the train_loss and test_loss attributes. predict(data_loader - DataLoader) -> tuple[torch.Tensor, torch.Tensor]: Evaluates the model on the given data loader. save( - model_name - str, - subfolder - str, - training_id - str, - data_params - dict, ) -> None: Saves the model to disk. load(training_id - str, surr_name: str, model_identifier: str) -> None: Loads a trained surrogate model. setup_progress_bar(epochs - int, position: int, description: str) -> tqdm: Helper function to set up a progress bar for training. denormalize(data - torch.Tensor) -> torch.Tensor: Denormalizes the data back to the original scale.","title":"Methods"},{"location":"technical/codes/surrogates/surrogates/#signature","text":"class AbstractSurrogateModel(ABC, nn.Module): def __init__( self, device: str | None = None, n_chemicals: int = 29, n_timesteps: int = 100, config: dict | None = None, ): ...","title":"Signature"},{"location":"technical/codes/surrogates/surrogates/#abstractsurrogatemodeldenormalize","text":"Show source in surrogates.py:362 Denormalize the data.","title":"AbstractSurrogateModel().denormalize"},{"location":"technical/codes/surrogates/surrogates/#arguments_1","text":"data np.ndarray - The data to denormalize.","title":"Arguments"},{"location":"technical/codes/surrogates/surrogates/#returns","text":"np.ndarray - The denormalized data.","title":"Returns"},{"location":"technical/codes/surrogates/surrogates/#signature_1","text":"def denormalize(self, data: torch.Tensor) -> torch.Tensor: ...","title":"Signature"},{"location":"technical/codes/surrogates/surrogates/#abstractsurrogatemodelfit","text":"Show source in surrogates.py:147 Perform the training of the model. Sets the train_loss and test_loss attributes.","title":"AbstractSurrogateModel().fit"},{"location":"technical/codes/surrogates/surrogates/#arguments_2","text":"train_loader DataLoader - The DataLoader object containing the training data. test_loader DataLoader - The DataLoader object containing the testing data. epochs int - The number of epochs to train the model for. position int - The position of the progress bar. description str - The description of the progress bar.","title":"Arguments"},{"location":"technical/codes/surrogates/surrogates/#signature_2","text":"@abstractmethod def fit( self, train_loader: DataLoader, test_loader: DataLoader, epochs: int, position: int, description: str, ) -> None: ...","title":"Signature"},{"location":"technical/codes/surrogates/surrogates/#abstractsurrogatemodelforward","text":"Show source in surrogates.py:106 Forward pass of the model.","title":"AbstractSurrogateModel().forward"},{"location":"technical/codes/surrogates/surrogates/#arguments_3","text":"inputs Any - The input data as recieved from the dataloader.","title":"Arguments"},{"location":"technical/codes/surrogates/surrogates/#returns_1","text":"tuple[Tensor, Tensor] - The model predictions and the targets.","title":"Returns"},{"location":"technical/codes/surrogates/surrogates/#signature_3","text":"@abstractmethod def forward(self, inputs: Any) -> tuple[Tensor, Tensor]: ...","title":"Signature"},{"location":"technical/codes/surrogates/surrogates/#abstractsurrogatemodelload","text":"Show source in surrogates.py:294 Load a trained surrogate model.","title":"AbstractSurrogateModel().load"},{"location":"technical/codes/surrogates/surrogates/#arguments_4","text":"training_id str - The training identifier. surr_name str - The name of the surrogate model. model_identifier str - The identifier of the model (e.g., 'main').","title":"Arguments"},{"location":"technical/codes/surrogates/surrogates/#returns_2","text":"None. The model is loaded in place.","title":"Returns"},{"location":"technical/codes/surrogates/surrogates/#signature_4","text":"def load( self, training_id: str, surr_name: str, model_identifier: str, model_dir: str | None = None, ) -> None: ...","title":"Signature"},{"location":"technical/codes/surrogates/surrogates/#abstractsurrogatemodelpredict","text":"Show source in surrogates.py:168 Evaluate the model on the given dataloader.","title":"AbstractSurrogateModel().predict"},{"location":"technical/codes/surrogates/surrogates/#arguments_5","text":"data_loader DataLoader - The DataLoader object containing the data the model is evaluated on.","title":"Arguments"},{"location":"technical/codes/surrogates/surrogates/#returns_3","text":"tuple[torch.Tensor, torch.Tensor] - The predictions and targets.","title":"Returns"},{"location":"technical/codes/surrogates/surrogates/#signature_5","text":"def predict(self, data_loader: DataLoader) -> tuple[torch.Tensor, torch.Tensor]: ...","title":"Signature"},{"location":"technical/codes/surrogates/surrogates/#abstractsurrogatemodelprepare_data","text":"Show source in surrogates.py:119 Prepare the data for training, testing, and validation. This method should return the DataLoader objects for the training, testing, and validation data.","title":"AbstractSurrogateModel().prepare_data"},{"location":"technical/codes/surrogates/surrogates/#arguments_6","text":"dataset_train np.ndarray - The training dataset. dataset_test np.ndarray - The testing dataset. dataset_val np.ndarray - The validation dataset. timesteps np.ndarray - The timesteps. batch_size int - The batch size. shuffle bool - Whether to shuffle the data.","title":"Arguments"},{"location":"technical/codes/surrogates/surrogates/#returns_4","text":"tuple[DataLoader, DataLoader, DataLoader]: The DataLoader objects for the training, testing, and validation data.","title":"Returns"},{"location":"technical/codes/surrogates/surrogates/#signature_6","text":"@abstractmethod def prepare_data( self, dataset_train: np.ndarray, dataset_test: np.ndarray | None, dataset_val: np.ndarray | None, timesteps: np.ndarray, batch_size: int, shuffle: bool, ) -> tuple[DataLoader, DataLoader | None, DataLoader | None]: ...","title":"Signature"},{"location":"technical/codes/surrogates/surrogates/#abstractsurrogatemodelsave","text":"Show source in surrogates.py:219 Save the model to disk.","title":"AbstractSurrogateModel().save"},{"location":"technical/codes/surrogates/surrogates/#arguments_7","text":"model_name str - The name of the model. subfolder str - The subfolder to save the model in. training_id str - The training identifier. data_params dict - The data parameters.","title":"Arguments"},{"location":"technical/codes/surrogates/surrogates/#signature_7","text":"def save( self, model_name: str, base_dir: str, training_id: str, data_params: dict ) -> None: ...","title":"Signature"},{"location":"technical/codes/surrogates/surrogates/#abstractsurrogatemodelsetup_progress_bar","text":"Show source in surrogates.py:337 Helper function to set up a progress bar for training.","title":"AbstractSurrogateModel().setup_progress_bar"},{"location":"technical/codes/surrogates/surrogates/#arguments_8","text":"epochs int - The number of epochs. position int - The position of the progress bar. description str - The description of the progress bar.","title":"Arguments"},{"location":"technical/codes/surrogates/surrogates/#returns_5","text":"tqdm - The progress bar.","title":"Returns"},{"location":"technical/codes/surrogates/surrogates/#signature_8","text":"def setup_progress_bar(self, epochs: int, position: int, description: str): ...","title":"Signature"},{"location":"technical/codes/train/","text":"Train Codes-benchmark Index / Codes / Train Auto-generated documentation for codes.train module. Train Modules Modules Train Fcts","title":"Train"},{"location":"technical/codes/train/#train","text":"Codes-benchmark Index / Codes / Train Auto-generated documentation for codes.train module. Train Modules","title":"Train"},{"location":"technical/codes/train/#modules","text":"Train Fcts","title":"Modules"},{"location":"technical/codes/train/train_fcts/","text":"Train Fcts Codes-benchmark Index / Codes / Train / Train Fcts Auto-generated documentation for codes.train.train_fcts module. Train Fcts create_task_list_for_surrogate parallel_training sequential_training train_and_save_model worker create_task_list_for_surrogate Show source in train_fcts.py:122 Creates a list of training tasks for a specific surrogate model based on the configuration file. Arguments config dict - The configuration dictionary taken from the config file. surr_name str - The name of the surrogate model. Returns list - A list of training tasks for the surrogate model. Signature def create_task_list_for_surrogate(config, surr_name: str) -> list: ... parallel_training Show source in train_fcts.py:207 Execute the training tasks in parallel on multiple devices. Arguments tasks list - The list of training tasks. device_list list - The list of devices to use for training. task_list_filepath str - The filepath to the task list file. Signature def parallel_training(tasks, device_list, task_list_filepath: str): ... sequential_training Show source in train_fcts.py:251 Execute the training tasks sequentially on a single device. Arguments tasks list - The list of training tasks. device_list list - The list of devices to use for training. task_list_filepath str - The filepath to the task list file. Signature def sequential_training(tasks, device_list, task_list_filepath: str): ... train_and_save_model Show source in train_fcts.py:19 Train and save a model for a specific benchmark mode. The parameters are determined by the task(s) which is created from the config file. Arguments surr_name str - The name of the surrogate model. mode str - The benchmark mode (e.g. \"main\", \"interpolation\", \"extrapolation\"). metric int - The metric for the benchmark mode. training_id str - The training ID for the current training session. seed int, optional - The random seed for the training. Defaults to None. epochs int, optional - The number of epochs for the training. Defaults to None. device str, optional - The device for the training. Defaults to \"cpu\". position int, optional - The position of the model in the task list. Defaults to 1. Signature def train_and_save_model( surr_name: str, mode: str, metric: int, training_id: str, seed: int | None = None, epochs: int | None = None, device: str = \"cpu\", position: int = 1, ): ... worker Show source in train_fcts.py:173 Worker function to process tasks from the task queue on the given device. Arguments task_queue Queue - The task queue containing the training tasks. device str - The device to use for training. device_idx int - The index of the device in the device list. overall_progress_bar tqdm - The overall progress bar for the training. task_list_filepath str - The filepath to the task list file Signature def worker( task_queue: Queue, device: str, device_idx: int, overall_progress_bar: tqdm, task_list_filepath: str, ): ...","title":"Train Fcts"},{"location":"technical/codes/train/train_fcts/#train-fcts","text":"Codes-benchmark Index / Codes / Train / Train Fcts Auto-generated documentation for codes.train.train_fcts module. Train Fcts create_task_list_for_surrogate parallel_training sequential_training train_and_save_model worker","title":"Train Fcts"},{"location":"technical/codes/train/train_fcts/#create_task_list_for_surrogate","text":"Show source in train_fcts.py:122 Creates a list of training tasks for a specific surrogate model based on the configuration file.","title":"create_task_list_for_surrogate"},{"location":"technical/codes/train/train_fcts/#arguments","text":"config dict - The configuration dictionary taken from the config file. surr_name str - The name of the surrogate model.","title":"Arguments"},{"location":"technical/codes/train/train_fcts/#returns","text":"list - A list of training tasks for the surrogate model.","title":"Returns"},{"location":"technical/codes/train/train_fcts/#signature","text":"def create_task_list_for_surrogate(config, surr_name: str) -> list: ...","title":"Signature"},{"location":"technical/codes/train/train_fcts/#parallel_training","text":"Show source in train_fcts.py:207 Execute the training tasks in parallel on multiple devices.","title":"parallel_training"},{"location":"technical/codes/train/train_fcts/#arguments_1","text":"tasks list - The list of training tasks. device_list list - The list of devices to use for training. task_list_filepath str - The filepath to the task list file.","title":"Arguments"},{"location":"technical/codes/train/train_fcts/#signature_1","text":"def parallel_training(tasks, device_list, task_list_filepath: str): ...","title":"Signature"},{"location":"technical/codes/train/train_fcts/#sequential_training","text":"Show source in train_fcts.py:251 Execute the training tasks sequentially on a single device.","title":"sequential_training"},{"location":"technical/codes/train/train_fcts/#arguments_2","text":"tasks list - The list of training tasks. device_list list - The list of devices to use for training. task_list_filepath str - The filepath to the task list file.","title":"Arguments"},{"location":"technical/codes/train/train_fcts/#signature_2","text":"def sequential_training(tasks, device_list, task_list_filepath: str): ...","title":"Signature"},{"location":"technical/codes/train/train_fcts/#train_and_save_model","text":"Show source in train_fcts.py:19 Train and save a model for a specific benchmark mode. The parameters are determined by the task(s) which is created from the config file.","title":"train_and_save_model"},{"location":"technical/codes/train/train_fcts/#arguments_3","text":"surr_name str - The name of the surrogate model. mode str - The benchmark mode (e.g. \"main\", \"interpolation\", \"extrapolation\"). metric int - The metric for the benchmark mode. training_id str - The training ID for the current training session. seed int, optional - The random seed for the training. Defaults to None. epochs int, optional - The number of epochs for the training. Defaults to None. device str, optional - The device for the training. Defaults to \"cpu\". position int, optional - The position of the model in the task list. Defaults to 1.","title":"Arguments"},{"location":"technical/codes/train/train_fcts/#signature_3","text":"def train_and_save_model( surr_name: str, mode: str, metric: int, training_id: str, seed: int | None = None, epochs: int | None = None, device: str = \"cpu\", position: int = 1, ): ...","title":"Signature"},{"location":"technical/codes/train/train_fcts/#worker","text":"Show source in train_fcts.py:173 Worker function to process tasks from the task queue on the given device.","title":"worker"},{"location":"technical/codes/train/train_fcts/#arguments_4","text":"task_queue Queue - The task queue containing the training tasks. device str - The device to use for training. device_idx int - The index of the device in the device list. overall_progress_bar tqdm - The overall progress bar for the training. task_list_filepath str - The filepath to the task list file","title":"Arguments"},{"location":"technical/codes/train/train_fcts/#signature_4","text":"def worker( task_queue: Queue, device: str, device_idx: int, overall_progress_bar: tqdm, task_list_filepath: str, ): ...","title":"Signature"},{"location":"technical/codes/utils/","text":"Utils Codes-benchmark Index / Codes / Utils Auto-generated documentation for codes.utils module. Utils Modules Modules Data Utils Utils","title":"Utils"},{"location":"technical/codes/utils/#utils","text":"Codes-benchmark Index / Codes / Utils Auto-generated documentation for codes.utils module. Utils Modules","title":"Utils"},{"location":"technical/codes/utils/#modules","text":"Data Utils Utils","title":"Modules"},{"location":"technical/codes/utils/data_utils/","text":"Data Utils Codes-benchmark Index / Codes / Utils / Data Utils Auto-generated documentation for codes.utils.data_utils module. Data Utils DatasetError check_and_load_data create_dataset create_hdf5_dataset download_data get_data_subset normalize_data DatasetError Show source in data_utils.py:10 Error for missing data or dataset or if the data shape is incorrect. Signature class DatasetError(Exception): ... check_and_load_data Show source in data_utils.py:18 Check the specified dataset and load the data based on the mode (train or test). Arguments dataset_name str - The name of the dataset. verbose bool - Whether to print information about the loaded data. log bool - Whether to log-transform the data (log10). normalisation_mode str - The normalization mode, either \"disable\", \"minmax\", or \"standardise\". tolerance float, optional - The tolerance value for log-transformation. Values below this will be set to the tolerance value. Pass None to disable. Returns tuple - Loaded data and timesteps. Raises DatasetError - If the dataset or required data is missing or if the data shape is incorrect. Signature def check_and_load_data( dataset_name: str, verbose: bool = True, log: bool = True, normalisation_mode: str = \"standardise\", tolerance: float | None = 1e-20, ): ... create_dataset Show source in data_utils.py:321 Creates a new dataset in the data directory. Arguments name str - The name of the dataset. train_data (np.ndarray | torch.Tensor): The training data. test_data (np.ndarray | torch.Tensor, optional): The test data. val_data (np.ndarray | torch.Tensor, optional): The validation data. split tuple(float, float, float), optional): If test_data and val_data are not provided, train_data can be split into training, test and validation data. timesteps (np.ndarray | torch.Tensor, optional): The timesteps array. labels list[str], optional - The labels for the chemicals. Raises FileExistsError - If the dataset already exists. TypeError - If the train_data is not a numpy array or torch tensor. ValueError - If the train_data, test_data, and val_data do not have the correct shape. Signature def create_dataset( name: str, train_data: np.ndarray, test_data: np.ndarray | None = None, val_data: np.ndarray | None = None, split: tuple[float, float, float] | None = None, timesteps: np.ndarray | None = None, labels: list[str] | None = None, ): ... create_hdf5_dataset Show source in data_utils.py:237 Create an HDF5 file for a dataset with train and test data, and optionally timesteps. Additionally, store metadata about the dataset. Arguments train_data np.ndarray - The training data array of shape (n_samples, n_timesteps, n_chemicals). test_data np.ndarray - The test data array of shape (n_samples, n_timesteps, n_chemicals). val_data np.ndarray - The validation data array of shape (n_samples, n_timesteps, n_chemicals). dataset_name str - The name of the dataset. data_dir str - The directory to save the dataset in. timesteps np.ndarray, optional - The timesteps array. If None, integer timesteps will be generated. labels list[str], optional - The labels for the chemicals. Signature def create_hdf5_dataset( train_data: np.ndarray, test_data: np.ndarray, val_data: np.ndarray, dataset_name: str, data_dir: str = \"datasets\", timesteps: np.ndarray | None = None, labels: list[str] | None = None, ): ... download_data Show source in data_utils.py:449 Download the specified dataset if it is not present Arguments dataset_name str - The name of the dataset. path str, optional - The path to save the dataset. If None, the default data directory is used. Signature def download_data(dataset_name: str, path: str | None = None): ... get_data_subset Show source in data_utils.py:282 Get the appropriate data subset based on the mode and metric. Arguments full_train_data np.ndarray - The full training data. full_test_data np.ndarray - The full test data. timesteps np.ndarray - The timesteps. mode str - The benchmark mode (e.g., \"accuracy\", \"interpolation\", \"extrapolation\", \"sparse\", \"UQ\"). metric int - The specific metric for the mode (e.g., interval, cutoff, factor, batch size). Returns tuple - The training data, test data, and timesteps subset. Signature def get_data_subset(full_train_data, full_test_data, timesteps, mode, metric): ... normalize_data Show source in data_utils.py:173 Normalize the data based on the training data statistics. Arguments train_data np.ndarray - Training data array. test_data np.ndarray, optional - Test data array. val_data np.ndarray, optional - Validation data array. mode str - Normalization mode, either \"minmax\" or \"standardise\". Returns tuple - Normalized training data, test data, and validation data. Signature def normalize_data( train_data: np.ndarray, test_data: np.ndarray | None = None, val_data: np.ndarray | None = None, mode: str = \"standardise\", ) -> tuple: ...","title":"Data Utils"},{"location":"technical/codes/utils/data_utils/#data-utils","text":"Codes-benchmark Index / Codes / Utils / Data Utils Auto-generated documentation for codes.utils.data_utils module. Data Utils DatasetError check_and_load_data create_dataset create_hdf5_dataset download_data get_data_subset normalize_data","title":"Data Utils"},{"location":"technical/codes/utils/data_utils/#dataseterror","text":"Show source in data_utils.py:10 Error for missing data or dataset or if the data shape is incorrect.","title":"DatasetError"},{"location":"technical/codes/utils/data_utils/#signature","text":"class DatasetError(Exception): ...","title":"Signature"},{"location":"technical/codes/utils/data_utils/#check_and_load_data","text":"Show source in data_utils.py:18 Check the specified dataset and load the data based on the mode (train or test).","title":"check_and_load_data"},{"location":"technical/codes/utils/data_utils/#arguments","text":"dataset_name str - The name of the dataset. verbose bool - Whether to print information about the loaded data. log bool - Whether to log-transform the data (log10). normalisation_mode str - The normalization mode, either \"disable\", \"minmax\", or \"standardise\". tolerance float, optional - The tolerance value for log-transformation. Values below this will be set to the tolerance value. Pass None to disable.","title":"Arguments"},{"location":"technical/codes/utils/data_utils/#returns","text":"tuple - Loaded data and timesteps.","title":"Returns"},{"location":"technical/codes/utils/data_utils/#raises","text":"DatasetError - If the dataset or required data is missing or if the data shape is incorrect.","title":"Raises"},{"location":"technical/codes/utils/data_utils/#signature_1","text":"def check_and_load_data( dataset_name: str, verbose: bool = True, log: bool = True, normalisation_mode: str = \"standardise\", tolerance: float | None = 1e-20, ): ...","title":"Signature"},{"location":"technical/codes/utils/data_utils/#create_dataset","text":"Show source in data_utils.py:321 Creates a new dataset in the data directory.","title":"create_dataset"},{"location":"technical/codes/utils/data_utils/#arguments_1","text":"name str - The name of the dataset. train_data (np.ndarray | torch.Tensor): The training data. test_data (np.ndarray | torch.Tensor, optional): The test data. val_data (np.ndarray | torch.Tensor, optional): The validation data. split tuple(float, float, float), optional): If test_data and val_data are not provided, train_data can be split into training, test and validation data. timesteps (np.ndarray | torch.Tensor, optional): The timesteps array. labels list[str], optional - The labels for the chemicals.","title":"Arguments"},{"location":"technical/codes/utils/data_utils/#raises_1","text":"FileExistsError - If the dataset already exists. TypeError - If the train_data is not a numpy array or torch tensor. ValueError - If the train_data, test_data, and val_data do not have the correct shape.","title":"Raises"},{"location":"technical/codes/utils/data_utils/#signature_2","text":"def create_dataset( name: str, train_data: np.ndarray, test_data: np.ndarray | None = None, val_data: np.ndarray | None = None, split: tuple[float, float, float] | None = None, timesteps: np.ndarray | None = None, labels: list[str] | None = None, ): ...","title":"Signature"},{"location":"technical/codes/utils/data_utils/#create_hdf5_dataset","text":"Show source in data_utils.py:237 Create an HDF5 file for a dataset with train and test data, and optionally timesteps. Additionally, store metadata about the dataset.","title":"create_hdf5_dataset"},{"location":"technical/codes/utils/data_utils/#arguments_2","text":"train_data np.ndarray - The training data array of shape (n_samples, n_timesteps, n_chemicals). test_data np.ndarray - The test data array of shape (n_samples, n_timesteps, n_chemicals). val_data np.ndarray - The validation data array of shape (n_samples, n_timesteps, n_chemicals). dataset_name str - The name of the dataset. data_dir str - The directory to save the dataset in. timesteps np.ndarray, optional - The timesteps array. If None, integer timesteps will be generated. labels list[str], optional - The labels for the chemicals.","title":"Arguments"},{"location":"technical/codes/utils/data_utils/#signature_3","text":"def create_hdf5_dataset( train_data: np.ndarray, test_data: np.ndarray, val_data: np.ndarray, dataset_name: str, data_dir: str = \"datasets\", timesteps: np.ndarray | None = None, labels: list[str] | None = None, ): ...","title":"Signature"},{"location":"technical/codes/utils/data_utils/#download_data","text":"Show source in data_utils.py:449 Download the specified dataset if it is not present","title":"download_data"},{"location":"technical/codes/utils/data_utils/#arguments_3","text":"dataset_name str - The name of the dataset. path str, optional - The path to save the dataset. If None, the default data directory is used.","title":"Arguments"},{"location":"technical/codes/utils/data_utils/#signature_4","text":"def download_data(dataset_name: str, path: str | None = None): ...","title":"Signature"},{"location":"technical/codes/utils/data_utils/#get_data_subset","text":"Show source in data_utils.py:282 Get the appropriate data subset based on the mode and metric.","title":"get_data_subset"},{"location":"technical/codes/utils/data_utils/#arguments_4","text":"full_train_data np.ndarray - The full training data. full_test_data np.ndarray - The full test data. timesteps np.ndarray - The timesteps. mode str - The benchmark mode (e.g., \"accuracy\", \"interpolation\", \"extrapolation\", \"sparse\", \"UQ\"). metric int - The specific metric for the mode (e.g., interval, cutoff, factor, batch size).","title":"Arguments"},{"location":"technical/codes/utils/data_utils/#returns_1","text":"tuple - The training data, test data, and timesteps subset.","title":"Returns"},{"location":"technical/codes/utils/data_utils/#signature_5","text":"def get_data_subset(full_train_data, full_test_data, timesteps, mode, metric): ...","title":"Signature"},{"location":"technical/codes/utils/data_utils/#normalize_data","text":"Show source in data_utils.py:173 Normalize the data based on the training data statistics.","title":"normalize_data"},{"location":"technical/codes/utils/data_utils/#arguments_5","text":"train_data np.ndarray - Training data array. test_data np.ndarray, optional - Test data array. val_data np.ndarray, optional - Validation data array. mode str - Normalization mode, either \"minmax\" or \"standardise\".","title":"Arguments"},{"location":"technical/codes/utils/data_utils/#returns_2","text":"tuple - Normalized training data, test data, and validation data.","title":"Returns"},{"location":"technical/codes/utils/data_utils/#signature_6","text":"def normalize_data( train_data: np.ndarray, test_data: np.ndarray | None = None, val_data: np.ndarray | None = None, mode: str = \"standardise\", ) -> tuple: ...","title":"Signature"},{"location":"technical/codes/utils/utils/","text":"Utils Codes-benchmark Index / Codes / Utils / Utils Auto-generated documentation for codes.utils.utils module. Utils check_training_status create_model_dir get_progress_bar load_and_save_config load_task_list make_description nice_print read_yaml_config save_task_list set_random_seeds time_execution worker_init_fn check_training_status Show source in utils.py:221 Check if the training is already completed by looking for a completion marker file. If the training is not complete, compare the configurations and ask for a confirmation if there are differences. Arguments config dict - The configuration dictionary. Returns str - The path to the task list file. bool - Whether to copy the configuration file. Signature def check_training_status(config: dict) -> tuple[str, bool]: ... create_model_dir Show source in utils.py:43 Create a directory based on a unique identifier inside a specified subfolder of the base directory. Arguments base_dir str - The base directory where the subfolder and unique directory will be created. subfolder str - The subfolder inside the base directory to include before the unique directory. unique_id str - A unique identifier to be included in the directory name. Returns str - The path of the created unique directory within the specified subfolder. Signature def create_model_dir( base_dir: str = \".\", subfolder: str = \"trained\", unique_id: str = \"\" ) -> str: ... get_progress_bar Show source in utils.py:159 Create a progress bar with a specific description. Arguments tasks list - The list of tasks to be executed. Returns tqdm - The created progress bar. Signature def get_progress_bar(tasks: list) -> tqdm: ... load_and_save_config Show source in utils.py:66 Load configuration from a YAML file and save a copy to the specified directory. Arguments config_path str - The path to the configuration YAML file. save bool - Whether to save a copy of the configuration file. Default is True. Returns dict - The loaded configuration dictionary. Signature def load_and_save_config( config_path: str = \"config.yaml\", save: bool = True ) -> dict: ... load_task_list Show source in utils.py:203 Load a list of tasks from a JSON file. Arguments filepath str - The path to the JSON file. Returns list - The loaded list of tasks Signature def load_task_list(filepath: str | None) -> list: ... make_description Show source in utils.py:134 Create a formatted description for the progress bar that ensures consistent alignment. Arguments mode str - The benchmark mode (e.g., \"accuracy\", \"interpolation\", \"extrapolation\", \"sparse\", \"UQ\"). device str - The device to use for training (e.g., 'cuda:0'). metric str - The specific metric for the mode (e.g., interval, cutoff, factor, batch size). surrogate_name str - The name of the surrogate model. Returns str - A formatted description string for the progress bar. Signature def make_description( mode: str, device: str, metric: str, surrogate_name: str ) -> str: ... nice_print Show source in utils.py:111 Print a message in a nicely formatted way with a fixed width. Arguments message str - The message to print. width int - The width of the printed box. Default is 80. Signature def nice_print(message: str, width: int = 80) -> None: ... read_yaml_config Show source in utils.py:15 Signature def read_yaml_config(config_path): ... save_task_list Show source in utils.py:191 Save a list of tasks to a JSON file. Arguments tasks list - The list of tasks to save. filepath str - The path to the JSON file. Signature def save_task_list(tasks: list, filepath: str) -> None: ... set_random_seeds Show source in utils.py:96 Set random seeds for reproducibility. Arguments seed int - The random seed to set. Signature def set_random_seeds(seed: int): ... time_execution Show source in utils.py:21 Decorator to time the execution of a function and store the duration as an attribute of the function. Arguments func callable - The function to be timed. Signature def time_execution(func): ... worker_init_fn Show source in utils.py:179 Initialize the random seed for each worker in PyTorch DataLoader. Arguments worker_id int - The worker ID. Signature def worker_init_fn(worker_id): ...","title":"Utils"},{"location":"technical/codes/utils/utils/#utils","text":"Codes-benchmark Index / Codes / Utils / Utils Auto-generated documentation for codes.utils.utils module. Utils check_training_status create_model_dir get_progress_bar load_and_save_config load_task_list make_description nice_print read_yaml_config save_task_list set_random_seeds time_execution worker_init_fn","title":"Utils"},{"location":"technical/codes/utils/utils/#check_training_status","text":"Show source in utils.py:221 Check if the training is already completed by looking for a completion marker file. If the training is not complete, compare the configurations and ask for a confirmation if there are differences.","title":"check_training_status"},{"location":"technical/codes/utils/utils/#arguments","text":"config dict - The configuration dictionary.","title":"Arguments"},{"location":"technical/codes/utils/utils/#returns","text":"str - The path to the task list file. bool - Whether to copy the configuration file.","title":"Returns"},{"location":"technical/codes/utils/utils/#signature","text":"def check_training_status(config: dict) -> tuple[str, bool]: ...","title":"Signature"},{"location":"technical/codes/utils/utils/#create_model_dir","text":"Show source in utils.py:43 Create a directory based on a unique identifier inside a specified subfolder of the base directory.","title":"create_model_dir"},{"location":"technical/codes/utils/utils/#arguments_1","text":"base_dir str - The base directory where the subfolder and unique directory will be created. subfolder str - The subfolder inside the base directory to include before the unique directory. unique_id str - A unique identifier to be included in the directory name.","title":"Arguments"},{"location":"technical/codes/utils/utils/#returns_1","text":"str - The path of the created unique directory within the specified subfolder.","title":"Returns"},{"location":"technical/codes/utils/utils/#signature_1","text":"def create_model_dir( base_dir: str = \".\", subfolder: str = \"trained\", unique_id: str = \"\" ) -> str: ...","title":"Signature"},{"location":"technical/codes/utils/utils/#get_progress_bar","text":"Show source in utils.py:159 Create a progress bar with a specific description.","title":"get_progress_bar"},{"location":"technical/codes/utils/utils/#arguments_2","text":"tasks list - The list of tasks to be executed.","title":"Arguments"},{"location":"technical/codes/utils/utils/#returns_2","text":"tqdm - The created progress bar.","title":"Returns"},{"location":"technical/codes/utils/utils/#signature_2","text":"def get_progress_bar(tasks: list) -> tqdm: ...","title":"Signature"},{"location":"technical/codes/utils/utils/#load_and_save_config","text":"Show source in utils.py:66 Load configuration from a YAML file and save a copy to the specified directory.","title":"load_and_save_config"},{"location":"technical/codes/utils/utils/#arguments_3","text":"config_path str - The path to the configuration YAML file. save bool - Whether to save a copy of the configuration file. Default is True.","title":"Arguments"},{"location":"technical/codes/utils/utils/#returns_3","text":"dict - The loaded configuration dictionary.","title":"Returns"},{"location":"technical/codes/utils/utils/#signature_3","text":"def load_and_save_config( config_path: str = \"config.yaml\", save: bool = True ) -> dict: ...","title":"Signature"},{"location":"technical/codes/utils/utils/#load_task_list","text":"Show source in utils.py:203 Load a list of tasks from a JSON file.","title":"load_task_list"},{"location":"technical/codes/utils/utils/#arguments_4","text":"filepath str - The path to the JSON file.","title":"Arguments"},{"location":"technical/codes/utils/utils/#returns_4","text":"list - The loaded list of tasks","title":"Returns"},{"location":"technical/codes/utils/utils/#signature_4","text":"def load_task_list(filepath: str | None) -> list: ...","title":"Signature"},{"location":"technical/codes/utils/utils/#make_description","text":"Show source in utils.py:134 Create a formatted description for the progress bar that ensures consistent alignment.","title":"make_description"},{"location":"technical/codes/utils/utils/#arguments_5","text":"mode str - The benchmark mode (e.g., \"accuracy\", \"interpolation\", \"extrapolation\", \"sparse\", \"UQ\"). device str - The device to use for training (e.g., 'cuda:0'). metric str - The specific metric for the mode (e.g., interval, cutoff, factor, batch size). surrogate_name str - The name of the surrogate model.","title":"Arguments"},{"location":"technical/codes/utils/utils/#returns_5","text":"str - A formatted description string for the progress bar.","title":"Returns"},{"location":"technical/codes/utils/utils/#signature_5","text":"def make_description( mode: str, device: str, metric: str, surrogate_name: str ) -> str: ...","title":"Signature"},{"location":"technical/codes/utils/utils/#nice_print","text":"Show source in utils.py:111 Print a message in a nicely formatted way with a fixed width.","title":"nice_print"},{"location":"technical/codes/utils/utils/#arguments_6","text":"message str - The message to print. width int - The width of the printed box. Default is 80.","title":"Arguments"},{"location":"technical/codes/utils/utils/#signature_6","text":"def nice_print(message: str, width: int = 80) -> None: ...","title":"Signature"},{"location":"technical/codes/utils/utils/#read_yaml_config","text":"Show source in utils.py:15","title":"read_yaml_config"},{"location":"technical/codes/utils/utils/#signature_7","text":"def read_yaml_config(config_path): ...","title":"Signature"},{"location":"technical/codes/utils/utils/#save_task_list","text":"Show source in utils.py:191 Save a list of tasks to a JSON file.","title":"save_task_list"},{"location":"technical/codes/utils/utils/#arguments_7","text":"tasks list - The list of tasks to save. filepath str - The path to the JSON file.","title":"Arguments"},{"location":"technical/codes/utils/utils/#signature_8","text":"def save_task_list(tasks: list, filepath: str) -> None: ...","title":"Signature"},{"location":"technical/codes/utils/utils/#set_random_seeds","text":"Show source in utils.py:96 Set random seeds for reproducibility.","title":"set_random_seeds"},{"location":"technical/codes/utils/utils/#arguments_8","text":"seed int - The random seed to set.","title":"Arguments"},{"location":"technical/codes/utils/utils/#signature_9","text":"def set_random_seeds(seed: int): ...","title":"Signature"},{"location":"technical/codes/utils/utils/#time_execution","text":"Show source in utils.py:21 Decorator to time the execution of a function and store the duration as an attribute of the function.","title":"time_execution"},{"location":"technical/codes/utils/utils/#arguments_9","text":"func callable - The function to be timed.","title":"Arguments"},{"location":"technical/codes/utils/utils/#signature_10","text":"def time_execution(func): ...","title":"Signature"},{"location":"technical/codes/utils/utils/#worker_init_fn","text":"Show source in utils.py:179 Initialize the random seed for each worker in PyTorch DataLoader.","title":"worker_init_fn"},{"location":"technical/codes/utils/utils/#arguments_10","text":"worker_id int - The worker ID.","title":"Arguments"},{"location":"technical/codes/utils/utils/#signature_11","text":"def worker_init_fn(worker_id): ...","title":"Signature"},{"location":"technical/datasets/data_analysis/","text":"Data Analysis Codes-benchmark Index / datasets / Data Analysis Auto-generated documentation for datasets.data_analysis module. Data Analysis Modules Modules Analyse Dataset Data Plots","title":"Data Analysis"},{"location":"technical/datasets/data_analysis/#data-analysis","text":"Codes-benchmark Index / datasets / Data Analysis Auto-generated documentation for datasets.data_analysis module. Data Analysis Modules","title":"Data Analysis"},{"location":"technical/datasets/data_analysis/#modules","text":"Analyse Dataset Data Plots","title":"Modules"},{"location":"technical/datasets/data_analysis/analyse_dataset/","text":"Analyse Dataset Codes-benchmark Index / datasets / Data Analysis / Analyse Dataset Auto-generated documentation for datasets.data_analysis.analyse_dataset module. Analyse Dataset main main Show source in analyse_dataset.py:14 Main function to analyse the dataset. It checks the dataset and loads the data. Signature def main(args): ...","title":"Analyse Dataset"},{"location":"technical/datasets/data_analysis/analyse_dataset/#analyse-dataset","text":"Codes-benchmark Index / datasets / Data Analysis / Analyse Dataset Auto-generated documentation for datasets.data_analysis.analyse_dataset module. Analyse Dataset main","title":"Analyse Dataset"},{"location":"technical/datasets/data_analysis/analyse_dataset/#main","text":"Show source in analyse_dataset.py:14 Main function to analyse the dataset. It checks the dataset and loads the data.","title":"main"},{"location":"technical/datasets/data_analysis/analyse_dataset/#signature","text":"def main(args): ...","title":"Signature"},{"location":"technical/datasets/data_analysis/data_plots/","text":"Data Plots Codes-benchmark Index / datasets / Data Analysis / Data Plots Auto-generated documentation for datasets.data_analysis.data_plots module. Data Plots plot_example_trajectories plot_example_trajectories_paper plot_example_trajectories Show source in data_plots.py:11 Plot example trajectories for the dataset. Arguments dataset_name str - The name of the dataset. data np.ndarray - The data to plot. timesteps np.ndarray - Timesteps array. num_chemicals int, optional - Number of chemicals to plot. Default is 10. labels list, optional - List of labels for the chemicals. save bool, optional - Whether to save the plot as a file. Signature def plot_example_trajectories( dataset_name: str, data: np.ndarray, timesteps: np.ndarray, num_chemicals: int = 10, labels: list[str] | None = None, save: bool = False, sample_idx: int = 0, log: bool = False, ) -> None: ... plot_example_trajectories_paper Show source in data_plots.py:88 Plot example trajectories for the dataset with two subplots, one showing 15 chemicals and another showing the remaining. Arguments dataset_name str - The name of the dataset. data np.ndarray - The data to plot. timesteps np.ndarray - Timesteps array. save bool, optional - Whether to save the plot as a file. sample_idx int, optional - Index of the sample to plot. labels list, optional - List of labels for the chemicals. Signature def plot_example_trajectories_paper( dataset_name: str, data: np.ndarray, timesteps: np.ndarray, save: bool = False, sample_idx: int = 0, labels: list[str] | None = None, ) -> None: ...","title":"Data Plots"},{"location":"technical/datasets/data_analysis/data_plots/#data-plots","text":"Codes-benchmark Index / datasets / Data Analysis / Data Plots Auto-generated documentation for datasets.data_analysis.data_plots module. Data Plots plot_example_trajectories plot_example_trajectories_paper","title":"Data Plots"},{"location":"technical/datasets/data_analysis/data_plots/#plot_example_trajectories","text":"Show source in data_plots.py:11 Plot example trajectories for the dataset.","title":"plot_example_trajectories"},{"location":"technical/datasets/data_analysis/data_plots/#arguments","text":"dataset_name str - The name of the dataset. data np.ndarray - The data to plot. timesteps np.ndarray - Timesteps array. num_chemicals int, optional - Number of chemicals to plot. Default is 10. labels list, optional - List of labels for the chemicals. save bool, optional - Whether to save the plot as a file.","title":"Arguments"},{"location":"technical/datasets/data_analysis/data_plots/#signature","text":"def plot_example_trajectories( dataset_name: str, data: np.ndarray, timesteps: np.ndarray, num_chemicals: int = 10, labels: list[str] | None = None, save: bool = False, sample_idx: int = 0, log: bool = False, ) -> None: ...","title":"Signature"},{"location":"technical/datasets/data_analysis/data_plots/#plot_example_trajectories_paper","text":"Show source in data_plots.py:88 Plot example trajectories for the dataset with two subplots, one showing 15 chemicals and another showing the remaining.","title":"plot_example_trajectories_paper"},{"location":"technical/datasets/data_analysis/data_plots/#arguments_1","text":"dataset_name str - The name of the dataset. data np.ndarray - The data to plot. timesteps np.ndarray - Timesteps array. save bool, optional - Whether to save the plot as a file. sample_idx int, optional - Index of the sample to plot. labels list, optional - List of labels for the chemicals.","title":"Arguments"},{"location":"technical/datasets/data_analysis/data_plots/#signature_1","text":"def plot_example_trajectories_paper( dataset_name: str, data: np.ndarray, timesteps: np.ndarray, save: bool = False, sample_idx: int = 0, labels: list[str] | None = None, ) -> None: ...","title":"Signature"}]}