# Global settings for the benchmark
training_id: "_cloud_finetuned"
surrogates: ["MultiONet", "FullyConnected", "LatentNeuralODE", "LatentPoly"]
batch_size: [35838, 35838, 717, 717] # [65536, 65536, 512, 512]
epochs: [8000, 8000, 8000, 8000] # [4096, 4096, 4096, 4096]
dataset: 
  name: cloud
  log10_transform: True
  normalise: minmax
  subset_factor: 1
  tolerance: 1e-25
  normalise_per_species: True
  log_timesteps: True
devices: ["cuda:3", "cuda:2", "cuda:3", "cuda:5", "cuda:6", "cuda:7", "cuda:8", "cuda:9"]
seed: 42
verbose: False
checkpoint: True

# Models to train
interpolation: 
  enabled: False
  intervals: [2, 3, 4, 5, 6, 7, 8, 9, 10]
extrapolation:
  enabled: False
  cutoffs: [50, 60, 70, 80, 90]
sparse: 
  enabled: False
  factors: [2, 4, 8, 16, 32]
batch_scaling:
  enabled: False
  sizes: [1/16, 1/8, 1/4, 1/2]
uncertainty: 
  enabled: False
  ensemble_size: 5  # Number of models for deep ensemble

# Evaluations during benchmark
iterative: False
losses: False
gradients: False
timing: True
compute: False
compare: True # Whether to compare the surrogates
